@online{2xRJ45ModularJack,
  title = {{{2xRJ45 Modular Jack}} - {{TE Connectivity}} 5-6610000-1 | {{3D CAD Model Library}} | {{GrabCAD}}},
  url = {https://grabcad.com/library/2xrj45-modular-jack-te-connectivity-5-6610000-1},
  urldate = {2022-05-15},
  keywords = {cad\_file},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\JBHDFMY6\\2xrj45-modular-jack-te-connectivity-5-6610000-1.html}
}

@online{3m175534X60FT3M,
  title = {1755-3/{{4X60FT}} | {{3M Temflex}} 1755 {{Cloth Tape}}, 18m x 19.1mm, {{Black}}, {{Rubber Finish}} | {{RS Components}}},
  author = {3M},
  url = {https://no.rs-online.com/web/p/duct-tapes/8409026/},
  urldate = {2022-05-16},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\BUIDBHMM\\8409026.html}
}

@online{9.solutionsSolutionsCameraKit,
  title = {9.Solutions {{Camera Kit}}},
  author = {9.Solutions},
  url = {https://www.thomannmusic.no/intl/9solutions_camera_kit.htm},
  urldate = {2022-05-16},
  abstract = {Camera bracket kitConsisting of:     1x Quick Mount for lightweight camera, 1x Quick-mount receiver to handle bar, 1x GoPro multi-tool},
  langid = {english},
  organization = {{Musikhaus Thomann}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\HRTSF9H2\\9solutions_camera_kit.html}
}

@online{AcceleratedGStreamer,
  title = {Accelerated {{GStreamer}}},
  url = {https://docs.nvidia.com/jetson/archives/l4t-archived/l4t-3231/index.html#page/Tegra%2520Linux%2520Driver%2520Package%2520Development%2520Guide%2Faccelerated_gstreamer.html%23wwpID0E0UM0HA},
  urldate = {2023-05-02},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\VMUTGN4W\\index.html}
}

@online{AcceleratedGStreamerJetson,
  title = {Accelerated {{GStreamer}} â€” {{Jetson Linux}}{$<$}br/{$>$}{{Developer Guide}} 34.1 Documentation},
  url = {https://docs.nvidia.com/jetson/archives/r34.1/DeveloperGuide/text/SD/Multimedia/AcceleratedGstreamer.html#video-format-conversion-with-gstreamer-1-0},
  urldate = {2023-05-01},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\D5HP3QEZ\\AcceleratedGstreamer.html}
}

@article{AcceleratedGStreamerUser,
  title = {Accelerated {{GStreamer User Guide}}},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\95E2EIPF\\Accelerated GStreamer User Guide.pdf}
}

@online{adlerAnswerHowCan2021,
  title = {Answer to "{{How}} Can Calculate Mpeg2/Crc32 in {{Python}}?"},
  author = {Adler, Mark},
  date = {2021-09-27},
  url = {https://stackoverflow.com/a/69340177},
  urldate = {2022-05-16}
}

@report{adlinkPCIeGIE7xSeries,
  type = {Datasheet},
  title = {{{PCIe-GIE7x Series}}},
  author = {Adlink},
  number = {3.1},
  url = {https://www.adlinktech.com/Products/Download.ashx?type=MDownload&isManual=yes&file=1727%5cPCIe-GIE7x_50-11177-2020_31.pdf},
  urldate = {2022-05-02},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\RPX7UPLD\\PCIe-GIE7x_50-11177-2020_31.pdf}
}

@online{adminCentOSRHELChrony,
  title = {{{CentOS}} / {{RHEL}} 7 : {{Chrony V}}/s {{NTP}} ({{Differences Between}} Ntpd and Chronyd) â€“ {{The Geek Diary}}},
  author = {{admin}},
  url = {https://www.thegeekdiary.com/centos-rhel-7-chrony-vs-ntp-differences-between-ntpd-and-chronyd/},
  urldate = {2022-05-20},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\84AKLFH5\\centos-rhel-7-chrony-vs-ntp-differences-between-ntpd-and-chronyd.html}
}

@online{agfAnswerCreatingSingleton2011,
  title = {Answer to "{{Creating}} a Singleton in {{Python}}"},
  author = {{agf}},
  date = {2011-07-23},
  url = {https://stackoverflow.com/a/6798042/11739109},
  urldate = {2023-04-27},
  organization = {{Stack Overflow}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\CQ5QXFJH\\creating-a-singleton-in-python.html}
}

@online{AGXXavier35,
  title = {{{AGX Xavier}} 35.1.0 Enable Pps Process Full Record - {{Programmer Sought}}},
  url = {https://blog.csdn.net/whr19970424/article/details/129824969},
  urldate = {2023-05-04},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\5QCDRPB4\\129824969.html}
}

@online{alexisAnswerCreatingIsolated2022,
  title = {Answer to "{{Creating}} an Isolated {{NTP}} Server with Only {{1PPS}} and No External "Time" Input ({{No GPS}})"},
  author = {Alexis},
  date = {2022-01-27},
  url = {https://unix.stackexchange.com/a/688159},
  urldate = {2022-05-20},
  organization = {{Unix \& Linux Stack Exchange}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\XUPDPTL9\\688159.html}
}

@online{alperenRtspclientsinkTestPipeline2021,
  type = {Forum post},
  title = {Rtspclientsink Test Pipeline from Command Line},
  author = {Alperen},
  date = {2021-09-08},
  url = {https://stackoverflow.com/q/69098720/11739109},
  urldate = {2023-04-10},
  organization = {{Stack Overflow}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\BY78RLDE\\rtspclientsink-test-pipeline-from-command-line.html}
}

@report{alphawireAlphaWireUnterminated2013,
  title = {Alpha {{Wire Unterminated}} to {{Unterminated Coaxial Cable}}, {{RG174}}/{{U}}, 50 \textbackslash{{Omega}}, 30m},
  author = {Alpha Wire},
  date = {2013},
  url = {https://docs.rs-online.com/1380/0900766b8139013d.pdf},
  urldate = {2022-05-26},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\UFI59PTE\\TOP106_GNSS_Antenna.pdf}
}

@online{amphenolrfT1121A1ND3G150AmphenolRF,
  title = {{{T1121A1-ND3G-1-50}} | {{Amphenol RF}}, {{TNC Connector}} | {{RS Components}}},
  author = {Amphenol RF},
  url = {https://no.rs-online.com/web/p/coaxial-connectors/1943235},
  urldate = {2022-05-26},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\QT6ZP6RZ\\0900766b8139013d.pdf;C\:\\Users\\emilm\\Zotero\\storage\\3SDRUQRY\\1943235.html}
}

@article{andreffRobotHandEyeCalibration2001,
  title = {Robot {{Hand-Eye Calibration Using Structure-from-Motion}}},
  author = {Andreff, Nicolas and Horaud, Radu and Espiau, Bernard},
  date = {2001-03},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {20},
  number = {3},
  pages = {228--248},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/02783640122067372},
  url = {http://journals.sagepub.com/doi/10.1177/02783640122067372},
  urldate = {2022-11-26},
  abstract = {In this paper, we propose a new flexible method for hand-eye calibration. The vast majority of existing hand-eye calibration techniques require a calibration rig that is used in conjunction with camera pose estimation methods. Instead, we combine structure-from-motion with known robot motions, and we show that the solution can be obtained in linear form. The latter solves for both the hand-eye parameters and the unknown scale factor inherent with structure-from-motion methods. The algebraic analysis that is made possible with such a linear formulation allows investigation of not only the well-known case of general screw motions but also of such singular motions as pure translations, pure rotations, and planar motions. In essence, the robot-mounted camera looks to an unknown rigid layout, tracks points over an image sequence, and estimates the camera-to-robot relationship. Such a self-calibration process is relevant for unmanned vehicles, robots working in remote places, and so forth. We conduct a large number of experiments that validate the quality of the method by comparing it with existing ones.},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\LGLHIY46\\Andreff et al. - 2001 - Robot Hand-Eye Calibration Using Structure-from-Mo.pdf}
}

@online{annieahujaweb2020LinkLocalAddress2022,
  title = {Link {{Local Address}}},
  author = {{annieahujaweb2020}},
  date = {2022-06-05T10:50:06+00:00},
  url = {https://www.geeksforgeeks.org/link-local-address/},
  urldate = {2023-05-02},
  abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
  langid = {american},
  organization = {{GeeksforGeeks}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\Y8AS97AW\\link-local-address.html}
}

@article{attaranRise3DPrinting2017,
  title = {The Rise of 3-{{D}} Printing: {{The}} Advantages of Additive Manufacturing over Traditional Manufacturing},
  shorttitle = {The Rise of 3-{{D}} Printing},
  author = {Attaran, Mohsen},
  date = {2017-09-01},
  journaltitle = {Business Horizons},
  shortjournal = {Business Horizons},
  volume = {60},
  number = {5},
  pages = {677--688},
  issn = {0007-6813},
  doi = {10.1016/j.bushor.2017.05.011},
  url = {https://www.sciencedirect.com/science/article/pii/S0007681317300897},
  urldate = {2022-05-18},
  abstract = {The use of additive manufacturing technologies in different industries has increased substantially during the past years. Henry Ford introduced the moving assembly line that enabled mass production of identical products in the 20th century. Currently, additive manufacturing enables and facilitates production of moderate to mass quantities of products that can be customized individually. Additive manufacturing technologies are opening new opportunities in terms of production paradigm and manufacturing possibilities. Manufacturing lead times will be reduced substantially, new designs will have shorter time to market, and customer demand will be met more quickly. This article identifies additive manufacturing implementation challenges, highlights its evolving technologies and trends and their impact on the world of tomorrow, discusses its advantages over traditional manufacturing, explores its impact on the supply chain, and investigates its transformative potential and impact on various industry segments.},
  langid = {english},
  keywords = {3-D printing,Additive manufacturing,Global supply chain,Mass customization,Rapid prototyping},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\GULETRD8\\S0007681317300897.html}
}

@online{AutonomousVehicleVisualization,
  title = {Autonomous {{Vehicle Visualization}}},
  url = {https://dash.gallery/dash-avs-explorer/},
  urldate = {2022-05-30},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\GLEKD4EB\\dash-avs-explorer.html}
}

@online{barathMAGSACMarginalizingSample2019,
  title = {{{MAGSAC}}: Marginalizing Sample Consensus},
  shorttitle = {{{MAGSAC}}},
  author = {Barath, Daniel and Noskova, Jana and Matas, Jiri},
  date = {2019-06-04},
  eprint = {1803.07469},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1803.07469},
  url = {http://arxiv.org/abs/1803.07469},
  urldate = {2022-11-27},
  abstract = {A method called, sigma-consensus, is proposed to eliminate the need for a user-defined inlier-outlier threshold in RANSAC. Instead of estimating the noise sigma, it is marginalized over a range of noise scales. The optimized model is obtained by weighted least-squares fitting where the weights come from the marginalization over sigma of the point likelihoods of being inliers. A new quality function is proposed not requiring sigma and, thus, a set of inliers to determine the model quality. Also, a new termination criterion for RANSAC is built on the proposed marginalization approach. Applying sigma-consensus, MAGSAC is proposed with no need for a user-defined sigma and improving the accuracy of robust estimation significantly. It is superior to the state-of-the-art in terms of geometric accuracy on publicly available real-world datasets for epipolar geometry (F and E) and homography estimation. In addition, applying sigma-consensus only once as a post-processing step to the RANSAC output always improved the model quality on a wide range of vision problems without noticeable deterioration in processing time, adding a few milliseconds. The source code is at https://github.com/danini/magsac.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\5MI52UHM\\Barath et al. - 2019 - MAGSAC marginalizing sample consensus.pdf;C\:\\Users\\emilm\\Zotero\\storage\\IVVW6BPQ\\1803.html}
}

@article{barfootAssociatingUncertaintyThreeDimensional2014,
  title = {Associating {{Uncertainty With Three-Dimensional Poses}} for {{Use}} in {{Estimation Problems}}},
  author = {Barfoot, Timothy D. and Furgale, Paul T.},
  date = {2014-06},
  journaltitle = {IEEE Transactions on Robotics},
  volume = {30},
  number = {3},
  pages = {679--693},
  issn = {1941-0468},
  doi = {10.1109/TRO.2014.2298059},
  abstract = {In this paper, we provide specific and practical approaches to associate uncertainty with 4 Ã—4 transformation matrices, which is a common representation for pose variables in 3-D space. We show constraint-sensitive means of perturbing transformation matrices using their associated exponential-map generators and demonstrate these tools on three simple-yet-important estimation problems: 1) propagating uncertainty through a compound pose change, 2) fusing multiple measurements of a pose (e.g., for use in pose-graph relaxation), and 3) propagating uncertainty on poses (and landmarks) through a nonlinear camera model. The contribution of the paper is the presentation of the theoretical tools, which can be applied in the analysis of many problems involving 3-D pose and point variables.},
  eventtitle = {{{IEEE Transactions}} on {{Robotics}}},
  keywords = {Compounds,Covariance matrices,Estimation,Exponential maps,homogeneous points,lie,matrix Lie groups,Noise,pose uncertainty,Probability density function,Robots,transformation matrices,Uncertainty},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\DECQ3XHB\\Barfoot and Furgale - 2014 - Associating Uncertainty With Three-Dimensional Pos.pdf;C\:\\Users\\emilm\\Zotero\\storage\\W4S24ZW9\\6727494.html}
}

@online{bastian35022AnswerParsingHevc2014,
  title = {Answer to "{{Parsing}} Hevc Bitstream"},
  author = {Bastian35022},
  date = {2014-07-28},
  url = {https://stackoverflow.com/a/24996040},
  urldate = {2023-01-25},
  organization = {{Stack Overflow}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\QAEY3ZS2\\parsing-hevc-bitstream.html}
}

@video{beneaterHowCRCsWork2019,
  entrysubtype = {video},
  title = {How Do {{CRCs}} Work?},
  editor = {{Ben Eater}},
  editortype = {director},
  date = {2019-04-28},
  url = {https://www.youtube.com/watch?v=izG7qT0EpBw},
  urldate = {2022-05-27},
  abstract = {CRC (cyclic redundancy check) is one of the most common methods of error detection. It uses some interesting mathematical tricks to guarantee that it can catch certain kinds of errors. How does it work? Support these videos on Patreon: https://www.patreon.com/beneater or https://eater.net/support for other ways to support. ------------------ Social media: Website: https://www.eater.net Twitter: https://twitter.com/ben\_eater Patreon: https://patreon.com/beneater Reddit: https://www.reddit.com/r/beneater Special thanks to these supporters for making this video possible: Ben Dyson Ben Kamens Ben Williams Brandon Stranzl Christopher Blackmon Debilu Krastas Eric Dynowski Gonzalo Belascuen Greg Stratton Jay Binks Jayne Gabriele Johnathan Roatch Jordan Scales Manne Moquist Michael Nicholas Moresco Nick Wrightsman Randy True Ric Allinson Sachin Chitale SonOfSofaman}
}

@video{beneaterSPISerialPeripheral2021,
  entrysubtype = {video},
  title = {{{SPI}}: {{The}} Serial Peripheral Interface},
  shorttitle = {{{SPI}}},
  editor = {{Ben Eater}},
  editortype = {director},
  date = {2021-09-04},
  url = {https://www.youtube.com/watch?v=MCi7dCBhVpQ},
  urldate = {2022-05-26},
  abstract = {More 6502 computer info: https://eater.net/6502 Here's the temperature sensor module used in this video: https://amzn.to/2Wye3Ex More info on the sensor: https://tiny.cc/bme280 Support these videos on Patreon: https://www.patreon.com/beneater or https://eater.net/support for other ways to support.    ------------------    Social media:  Website: https://www.eater.net  Twitter: https://twitter.com/ben\_eater  Patreon: https://patreon.com/beneater  Reddit: https://www.reddit.com/r/beneater    Special thanks to these supporters for making this video possible: Aleksey Smolenchuk, Anders Carlsson, Andrew C. Young, Anson VanDoren, Anthanasius, anula, Armin Brauns, Ben, Ben Cochran, Ben Kamens, Ben Williams, Benny Olsson, Bill Cooksey, Binh Tran, Bouke Groenescheij, Bradley Pirtle, Bradley Stach, Brian T Hoover, Bryan Brickman, Burt Humburg, Carlos Ambrozak, Chris, Christian Carter, Christopher Blackmon, Dale Andrew Darling, Daniel Jeppsson, Daniel Tang, Dave Burley, Dave Walter, David Brown, David Clark, David Cox, David Dawkins, David House, David Sastre Medina, David Turner, David Worsham, Dean Bevan, Dean Winger, Dilip Gowda, Dissy, dko, Dmitry Guyvoronsky, DuÅ¡an DÅ¾elebdÅ¾iÄ‡, Dzevad Trumic, Emilio Mendoza, Eric Dynowski, Erik Broeders, Eugene Bulkin, Evan Thayer, Eveli LÃ¡szlÃ³, George Miroshnykov, Gonzalo Diaz, Harry McDow, hotwire33, Ingo Eble, Ivan Sorokin, James Capuder, james schaefer, Jared Dziedzic, Jason DeStefano, Jason Specland, JavaXP, Jaxon Ketterman, Jay Binks, Jayne Gabriele, Jeremy, Jeremy Cole, Jesse Miller, Jim Kelly, Jim Knowler, Jim Van Meggelen, Joe Beda, Joe OConnor, Joe Pregracke, Joel Miller, John Fenwick, John Hamberger jn., John Meade, Jon Dugan, Joseph Portaro, Joshua King, JurÄ£is Brigmanis, Kai Wells, Kefen, Kenneth Christensen, Kitick, Koreo, Lambda GPU Workstations, Larry, LÃ¡szlÃ³ BÃ¡csi, Lucky Resistor, Lukasz Pacholik, Marcos Fujisawa, Marcus Classon, Mark Day, Marko Clemente, Martin Noble, Martin Roth, Mats Fredriksson, Matt Krueger, MatthÃ¤us Pawelczyk, Matthew Duphily, Max Gawletta, Maxim Hansen, melvin2001, Michael Tedder, Michael Timbrook, Michael Weitman, Miguel RÃ­os, mikebad, Mikel Lindsaar, Miles Macchiaroli, Muqeet Mujahid, My Yiddishe Mama, Nicholas Counts, Nicholas Moresco, Not Yet Wise, Ã–rn Arnarson, Paul Pluzhnikov, Paul Randal, Pete Dietl, Phil Dennis, Philip Hofstetter, Phillip Glau, PixelSergey, Porus, ProgrammerDor, Randal Masutani, Randy True, raoulvp, Renaldas Zioma, Ric King, Richard Ertel, Rick Hennigan, Robert Comyn, Robert Diaz, Robey Pointer, Roland Bobek,Â§Ã§Ä«Å¤Ã¸ÅŸHÃ¯ Å‡Ã¥Ä¶Ä…Ã½ÅbÅ•Ã”,  Scott Holmes, Sean Patrick Oâ€™Brien, Sergey Kruk, Shelton, SonOfSofaman, Stefan Nesinger, Stefanus Du Toit, Stephen, Stephen Kovalcik, Stephen Riley, Stephen Smithstone, Steve  Jones, Tayler Porter, TheWebMachine, Thomas Bruggink, Thomas Eriksen, Tim Walkowski, Tom, Tom Yedwab, Tommaso Palmieri, Tyler Latham, Vincent Bernat, Walter Montalvo, Warren Miller, William, Wim Coekaerts, Wraithan McCarroll, xisente, Yee Lam Wan}
}

@online{biltemaAkrylplastOpalhvit,
  title = {Akrylplast, opalhvit},
  author = {{biltema}},
  url = {https://www.biltema.no/bygg/platematerialer/akryplast/akrylplast-opalhvit-2000043756},
  urldate = {2022-05-18},
  abstract = {Akrylplast, opalhvit},
  langid = {norwegianbokmal},
  keywords = {material},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\JJ45DUBK\\akrylplast-opalhvit-2000043756.html}
}

@online{birchardIntegratePlotlyDash2018,
  title = {Integrate {{Plotly Dash Into Your Flask App}}},
  author = {Birchard, Todd},
  date = {2018-12-10T14:58:00.000-05:00},
  url = {https://hackersandslackers.com/plotly-dash-with-flask/},
  urldate = {2022-05-30},
  abstract = {Use a clever workaround to embed interactive Plotly Dash interfaces into your Flask applications.},
  langid = {english},
  organization = {{Hackers and Slackers}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\6FCFVQB3\\plotly-dash-with-flask.html}
}

@software{bonghiJtopRbonghiJetson,
  title = {Jtop Â· Rbonghi/Jetson\_stats {{Wiki}}},
  author = {Bonghi, Raffaello},
  url = {https://github.com/rbonghi/jetson_stats},
  urldate = {2022-05-20},
  abstract = {ðŸ“Š Simple package for monitoring and control your NVIDIA Jetson [Xavier NX, Nano, AGX Xavier, TX1, TX2] - jtop Â· rbonghi/jetson\_stats Wiki}
}

@online{booplaEnclosure,
  title = {Enclosure},
  author = {BOOPLA},
  url = {https://www.bopla.de/en/enclosure-technology/product/bocube/pc-ul-94-v0-crystal-clear-lid/b-261712-pc-v0-g-7024.html},
  urldate = {2022-05-17},
  abstract = {Here you will find information about the BOPLA product range!},
  langid = {english},
  organization = {{BOPLA}},
  keywords = {cad\_file},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\XTV59JPI\\b-261712-pc-v0-g-7024.html}
}

@online{boplaBocubeEnclosure,
  title = {Bocube Enclosure},
  author = {BOPLA},
  url = {https://www.bopla.de/en/enclosure-technology/product/bocube/pc-ul-94-v0-crystal-clear-lid/b-261712-pc-v0-g-7024.html},
  urldate = {2022-05-18},
  abstract = {Here you will find information about the BOPLA product range!},
  langid = {english},
  organization = {{BOPLA}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\EA43NXDU\\b-261712-pc-v0-g-7024.html}
}

@online{brachmannLimitsPseudoGround2021,
  title = {On the {{Limits}} of {{Pseudo Ground Truth}} in {{Visual Camera Re-localisation}}},
  author = {Brachmann, Eric and Humenberger, Martin and Rother, Carsten and Sattler, Torsten},
  date = {2021-09-01},
  eprint = {2109.00524},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2109.00524},
  url = {http://arxiv.org/abs/2109.00524},
  urldate = {2022-11-27},
  abstract = {Benchmark datasets that measure camera pose accuracy have driven progress in visual re-localisation research. To obtain poses for thousands of images, it is common to use a reference algorithm to generate pseudo ground truth. Popular choices include Structure-from-Motion (SfM) and Simultaneous-Localisation-and-Mapping (SLAM) using additional sensors like depth cameras if available. Re-localisation benchmarks thus measure how well each method replicates the results of the reference algorithm. This begs the question whether the choice of the reference algorithm favours a certain family of re-localisation methods. This paper analyzes two widely used re-localisation datasets and shows that evaluation outcomes indeed vary with the choice of the reference algorithm. We thus question common beliefs in the re-localisation literature, namely that learning-based scene coordinate regression outperforms classical feature-based methods, and that RGB-D-based methods outperform RGB-based methods. We argue that any claims on ranking re-localisation methods should take the type of the reference algorithm, and the similarity of the methods to the reference algorithm, into account.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\W39ICD63\\Brachmann et al. - 2021 - On the Limits of Pseudo Ground Truth in Visual Cam.pdf;C\:\\Users\\emilm\\Zotero\\storage\\QCXT4DWM\\2109.html}
}

@online{brachmannVisualCameraReLocalization2020,
  title = {Visual {{Camera Re-Localization}} from {{RGB}} and {{RGB-D Images Using DSAC}}},
  author = {Brachmann, Eric and Rother, Carsten},
  date = {2020-10-09},
  eprint = {2002.12324},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2002.12324},
  urldate = {2022-11-27},
  abstract = {We describe a learning-based system that estimates the camera position and orientation from a single input image relative to a known environment. The system is flexible w.r.t. the amount of information available at test and at training time, catering to different applications. Input images can be RGB-D or RGB, and a 3D model of the environment can be utilized for training but is not necessary. In the minimal case, our system requires only RGB images and ground truth poses at training time, and it requires only a single RGB image at test time. The framework consists of a deep neural network and fully differentiable pose optimization. The neural network predicts so called scene coordinates, i.e. dense correspondences between the input image and 3D scene space of the environment. The pose optimization implements robust fitting of pose parameters using differentiable RANSAC (DSAC) to facilitate end-to-end training. The system, an extension of DSAC++ and referred to as DSAC*, achieves state-of-the-art accuracy an various public datasets for RGB-based re-localization, and competitive accuracy for RGB-D-based re-localization.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\ESU6RZG4\\Brachmann and Rother - 2020 - Visual Camera Re-Localization from RGB and RGB-D I.pdf;C\:\\Users\\emilm\\Zotero\\storage\\I466SH3A\\2002.12324.pdf;C\:\\Users\\emilm\\Zotero\\storage\\XBTGBS6E\\2002.html}
}

@book{brekkeFundamentalsSensorFusion2021,
  title = {Fundamentals of {{Sensor Fusion Target}} Tracking, Navigation and {{SLAM}}},
  author = {Brekke, Edmund},
  date = {2021-08-02},
  edition = {Third edition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\AL5NK7CA\\Brekke - 2021 - Fundamentals of Sensor Fusion Target tracking, nav.pdf}
}

@article{broekmanLowcostMobileRealtime2021,
  title = {A Low-Cost, Mobile Real-Time Kinematic Geolocation Service for Engineering and Research Applications},
  author = {Broekman, AndrÃ© and GrÃ¤be, Petrus Johannes},
  date = {2021-10},
  journaltitle = {HardwareX},
  shortjournal = {HardwareX},
  volume = {10},
  pages = {e00203},
  issn = {24680672},
  doi = {10.1016/j.ohx.2021.e00203},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2468067221000328},
  urldate = {2022-05-21},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\L8MTDAEH\\Broekman and GrÃ¤be - 2021 - A low-cost, mobile real-time kinematic geolocation.pdf}
}

@online{brossardAssociatingUncertaintyExtended2021,
  title = {Associating {{Uncertainty}} to {{Extended Poses}} for on {{Lie Group IMU Preintegration}} with {{Rotating Earth}}},
  author = {Brossard, Martin and Barrau, Axel and Chauchat, Paul and Bonnabel, SilvÃ¨re},
  date = {2021-01-08},
  eprint = {2007.14097},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2007.14097},
  url = {http://arxiv.org/abs/2007.14097},
  urldate = {2022-10-27},
  abstract = {The recently introduced matrix group SE2(3) provides a 5x5 matrix representation for the orientation, velocity and position of an object in the 3-D space, a triplet we call "extended pose". In this paper we build on this group to develop a theory to associate uncertainty with extended poses represented by 5x5 matrices. Our approach is particularly suited to describe how uncertainty propagates when the extended pose represents the state of an Inertial Measurement Unit (IMU). In particular it allows revisiting the theory of IMU preintegration on manifold and reaching a further theoretic level in this field. Exact preintegration formulas that account for rotating Earth, that is, centrifugal force and Coriolis force, are derived as a byproduct, and the factors are shown to be more accurate. The approach is validated through extensive simulations and applied to sensor-fusion where a loosely-coupled fixed-lag smoother fuses IMU and LiDAR on one hour long experiments using our experimental car. It shows how handling rotating Earth may be beneficial for long-term navigation within incremental smoothing algorithms.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,lie},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\YB3IRT66\\Brossard et al. - 2021 - Associating Uncertainty to Extended Poses for on L.pdf;C\:\\Users\\emilm\\Zotero\\storage\\3Y37TY2P\\2007.html}
}

@article{camposORBSLAM3AccurateOpenSource2021,
  title = {{{ORB-SLAM3}}: {{An Accurate Open-Source Library}} for {{Visual}}, {{Visual}}â€“{{Inertial}}, and {{Multimap SLAM}}},
  shorttitle = {{{ORB-SLAM3}}},
  author = {Campos, Carlos and Elvira, Richard and Rodriguez, Juan J. Gomez and M. Montiel, Jose M. and D. Tardos, Juan},
  date = {2021-12},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {37},
  number = {6},
  pages = {1874--1890},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2021.3075644},
  url = {https://ieeexplore.ieee.org/document/9440682/},
  urldate = {2022-11-16},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\J99ZCBC4\\Campos et al. - 2021 - ORB-SLAM3 An Accurate Open-Source Library for Vis.pdf}
}

@online{CanEnablePPS2023,
  title = {Can't Enable {{PPS}} on {{Jetson AGX Xavier}}},
  date = {2023-02-23T10:51:01+00:00},
  url = {https://forums.developer.nvidia.com/t/cant-enable-pps-on-jetson-agx-xavier/243842},
  urldate = {2023-05-04},
  abstract = {Hi everyone,  Iâ€™m trying to enable PPS on jetson AGX Xavier on the pin 24 from the header pins. I have a ublox SIMPLERTK2B gps connected with usb, timepulse pin connected to pin 24 and grounds are also connected. Also the jetson runs Jetpack 5.1 and the l4t driver Iâ€™ve downloaded is Jetson Linux 35.2.1  I have followed this guide to build the custom kernel. The changes Iâ€™ve made to the kernel are based on this and this.  More specifically I have made these changes to the kernel config  \# PPS sup...},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\CYFELSF6\\243842.html}
}

@online{canonicalltdUbuntuManpageEtc2019,
  title = {Ubuntu {{Manpage}}: /Etc/Modules - Kernel Modules to Load at Boot Time},
  author = {Canonical Ltd},
  date = {2019},
  url = {http://manpages.ubuntu.com/manpages/bionic/man5/modules.5.html},
  urldate = {2022-05-20},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\XL7E4M25\\modules.5.html}
}

@software{caudlePythonSpidev2022,
  title = {Python {{Spidev}}},
  author = {Caudle, Stephen},
  date = {2022-05-03T18:16:11Z},
  origdate = {2012-02-08T05:46:09Z},
  url = {https://github.com/doceme/py-spidev},
  urldate = {2022-05-27}
}

@online{chili555AnswerAutomaticallyConnect2014,
  title = {Answer to "{{Automatically}} Connect to a Wireless Network Using {{CLI}}"},
  author = {{chili555}},
  date = {2014-01-29},
  url = {https://askubuntu.com/a/412394},
  urldate = {2022-05-20},
  organization = {{Ask Ubuntu}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\IL3THPE9\\412394.html}
}

@software{ChronyIntroduction2021,
  title = {Chrony â€“ {{Introduction}}},
  date = {2021-12-16},
  url = {https://chrony.tuxfamily.org/},
  urldate = {2022-05-20},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\926GA7U6\\chrony.tuxfamily.org.html}
}

@inproceedings{chumLocallyOptimizedRANSAC2003,
  title = {Locally {{Optimized RANSAC}}},
  booktitle = {Pattern {{Recognition}}},
  author = {Chum, OndÅ™ej and Matas, JiÅ™Ã­ and Kittler, Josef},
  editor = {Michaelis, Bernd and Krell, Gerald},
  date = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {236--243},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-45243-0_31},
  abstract = {A new enhancement of ransac, the locally optimized ransac (lo-ransac), is introduced. It has been observed that, to find an optimal solution (with a given probability), the number of samples drawn in ransac is significantly higher than predicted from the mathematical model. This is due to the incorrect assumption, that a model with parameters computed from an outlier-free sample is consistent with all inliers. The assumption rarely holds in practice. The locally optimized ransac makes no new assumptions about the data, on the contrary â€“ it makes the above-mentioned assumption valid by applying local optimization to the solution estimated from the random sample.},
  isbn = {978-3-540-45243-0},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\2LK8LXWI\\Chum et al. - 2003 - Locally Optimized RANSAC.pdf}
}

@online{ConstantMemoryGPU,
  title = {Constant {{Memory}} â€“ {{GPU Programming}}},
  url = {https://carpentries-incubator.github.io/lesson-gpu-programming/08-constant_memory/index.html},
  urldate = {2023-04-24},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\AQQNX3TA\\index.html}
}

@online{CorsairMP600PRO,
  title = {Corsair MP600 PRO NH NVMe M.2 SSD 8TB - SSD M.2},
  url = {https://www.komplett.no/product/1221581/datautstyr/lagring/harddiskerssd/ssd-m2/corsair-mp600-pro-nh-nvme-m2-ssd-8tb},
  urldate = {2023-04-26},
  abstract = {Corsair MP600 PRO NH NVMe M.2 SSD 8TB - Opptil 7000MB/s Lese / Opptil 6100MB/s Skrive, PCIe Gen 4.0 x4, No Heatsink},
  langid = {norsk},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\CCKRUYUD\\corsair-mp600-pro-nh-nvme-m2-ssd-8tb.html}
}

@software{crostonRPiGPIOModule2022,
  title = {{{RPi}}.{{GPIO}}: {{A}} Module to Control {{Raspberry Pi GPIO}} Channels},
  shorttitle = {{{RPi}}.{{GPIO}}},
  author = {Croston, Ben},
  date = {2022-02-06},
  url = {http://sourceforge.net/projects/raspberry-gpio-python/},
  urldate = {2022-05-27},
  version = {0.7.1},
  keywords = {GPIO,Home Automation,{Pi,},{Raspberry,},Software Development,System - Hardware},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\A3YQBN9V\\RPi.GPIO.html}
}

@article{CUDAProgrammingGuide,
  title = {{{CUDA C}}++ {{Programming Guide}}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\7GA7K3YR\\CUDA C++ Programming Guide.pdf}
}

@online{CUDATegra,
  title = {{{CUDA}} for {{Tegra}}},
  url = {https://docs.nvidia.com/cuda/cuda-for-tegra-appnote/index.html#memory-selection%5B/url%5D},
  urldate = {2023-04-26},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\FVARR3BG\\index.html}
}

@online{danQueueingLinuxNetwork2013,
  title = {Queueing in the {{Linux Network Stack}} | {{Linux Journal}}},
  author = {Dan, Siemon},
  date = {2013-09-23},
  url = {https://www.coverfire.com/articles/queueing-in-the-linux-network-stack/comment-page-2/},
  urldate = {2023-04-27},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\9B65CA5S\\queueing-linux-network-stack.html}
}

@online{DebayeringVPI2021,
  title = {Debayering with {{VPI}}},
  date = {2021-09-08T13:14:11+00:00},
  url = {https://forums.developer.nvidia.com/t/debayering-with-vpi/188730},
  urldate = {2023-03-23},
  abstract = {Iâ€™ve been digging in to the new VPI API and Iâ€™ve been looking for a way to convert a bayered image (RGGB8) to an RGB8 or BGR8 format. I canâ€™t seem to find any bayered image formats in the VPI API.  Is there any example code or any way to debayer/demosaic a raw image with the VPI interface. It seems like most of the conversions available are between RGB/YUV/grayscale. Am I missing something? Or is the VPI API not currently capable of running an accelerated conversion from RGGB8 to RGB8/BGR8?},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\SNBEJ6ES\\188730.html}
}

@online{DifferentCRCAlgorithms,
  title = {9. {{Different CRC}} Algorithms},
  url = {https://pidlaboratory.com/9-rozne-algorytmy-crc/},
  urldate = {2022-05-16},
  abstract = {How does CRC work? Chapter~ 9 Different CRC algorithms Chapter. 9.1 Introduction The chapter 4 CRC arithmetic is a all CRC algorithms base. 1 bit shifts algorithm is realised on registersâ€“{$>$}chapter 5Read More...},
  langid = {british},
  organization = {{How does it work? Automatics, computers, etc...}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\Q2ZQ8JID\\9-rozne-algorytmy-crc.html}
}

@online{DigitalLowVoltage,
  title = {Digital {{Low Voltage Protector Disconnect Switch Cut Off 12V Over-Discharge Protection Module}} for 12-{{36V Lead Acid Lithium Battery}} : {{Patio}}, {{Lawn}} \& {{Garden}}},
  url = {https://www.amazon.com/Digital-Battery-Low-Voltage-Protection/dp/B07929Y5SZ},
  urldate = {2022-06-06},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\7P3KBDWJ\\B07929Y5SZ.html}
}

@report{diodesincorperated74HC32Datasheet2013,
  type = {Datasheet},
  title = {{{74HC32}} Datasheet},
  author = {DIODES incorperated},
  date = {2013-01},
  number = {DS35324 Rev. 3 - 2},
  url = {https://www.diodes.com/assets/Datasheets/74HC32.pdf},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\R3HYM9WL\\74HC32.pdf}
}

@online{dockerinc.DockerOverview2022,
  title = {Docker Overview},
  author = {Docker Inc.},
  date = {2022-05-18T13:19:31+00:00},
  url = {https://docs.docker.com/get-started/overview/},
  urldate = {2022-05-19},
  abstract = {Docker explained in depth},
  langid = {english},
  organization = {{Docker Documentation}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\P3APWQ67\\overview.html}
}

@software{dorsselaerUsbipdwin2022,
  title = {Usbipd-Win},
  author = {family=Dorsselaer, given=Frans, prefix=van, useprefix=false},
  date = {2022-05-18T19:34:55Z},
  origdate = {2020-10-18T21:44:11Z},
  url = {https://github.com/dorssel/usbipd-win},
  urldate = {2022-05-19},
  abstract = {Windows software for sharing locally connected USB devices to other machines, including Hyper-V guests and WSL 2.},
  keywords = {hyper-v,usb,usbip,usbip-win,usbipd,windows,wsl,wsl2}
}

@software{dorsselaerUsbipdwin2023,
  title = {Usbipd-Win},
  author = {family=Dorsselaer, given=Frans, prefix=van, useprefix=false},
  date = {2023-05-04T10:57:41Z},
  origdate = {2020-10-18T21:44:11Z},
  url = {https://github.com/dorssel/usbipd-win},
  urldate = {2023-05-04},
  abstract = {Windows software for sharing locally connected USB devices to other machines, including Hyper-V guests and WSL 2.},
  keywords = {hyper-v,usb,usbip,usbip-win,usbipd,windows,wsl,wsl2}
}

@online{drewbatgit10bit16bitYUV2022,
  title = {10-Bit and 16-Bit {{YUV Video Formats}} - {{Win32}} Apps},
  author = {{drewbatgit}},
  date = {2022-11-07},
  url = {https://learn.microsoft.com/en-us/windows/win32/medfound/10-bit-and-16-bit-yuv-video-formats},
  urldate = {2023-04-13},
  abstract = {This topic describes the 10- and 16-bit YUV formats that are recommended for capturing, processing, and displaying video in the Microsoft Windows operating system.},
  langid = {american},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\D8GSD28H\\10-bit-and-16-bit-yuv-video-formats.html}
}

@online{duaneAnswerRtspclientsinkTest2022,
  title = {Answer to "Rtspclientsink Test Pipeline from Command Line"},
  author = {Duane},
  date = {2022-07-31},
  url = {https://stackoverflow.com/a/73186851/11739109},
  urldate = {2023-04-10},
  organization = {{Stack Overflow}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\JJDRV7X8\\rtspclientsink-test-pipeline-from-command-line.html}
}

@online{DXFLaserFusion,
  title = {{{DXF}} for {{Laser}} | {{Fusion}} 360 | {{Autodesk App Store}}},
  url = {https://apps.autodesk.com/FUSION/en/Detail/Index?id=7634902334100976871&os=Win64&appLang=en},
  urldate = {2022-05-15},
  keywords = {cad\_file},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\DTB5TX7Z\\Index.html}
}

@online{easycomposites30mm27mmWoven,
  title = {30mm (27mm) {{Woven Finish Carbon Fibre Tube}}; 1m, 2m - {{Easy Composites}}},
  author = {Easy Composites},
  url = {https://www.easycomposites.co.uk/30mm-woven-finish-carbon-fibre-tube},
  urldate = {2022-05-16},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\8XRB8Y85\\30mm-woven-finish-carbon-fibre-tube.html}
}

@online{edgewallsoftwareRTCMNtrip,
  title = {{{RTCM-Ntrip}}},
  author = {Edgewall Software},
  url = {https://software.rtcm-ntrip.org/},
  urldate = {2022-05-02},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\MTL5IZHQ\\software.rtcm-ntrip.org.html}
}

@online{EfficientVariantsICP,
  title = {Efficient {{Variants}} of the {{ICP Algorithm}} | {{Request PDF}}},
  url = {https://www.researchgate.net/publication/3897594_Efficient_Variants_of_the_ICP_Algorithm},
  urldate = {2022-11-27},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\NQNZLABX\\Efficient Variants of the ICP Algorithm  Request .pdf;C\:\\Users\\emilm\\Zotero\\storage\\99E94HDG\\3897594_Efficient_Variants_of_the_ICP_Algorithm.html}
}

@online{ekinssolutionsllcNiftyDogbonesFusion,
  title = {Nifty {{Dogbones}} for {{Fusion}} 360},
  author = {Ekins Solutions LLC},
  url = {https://ekinssolutions.com/product/nifty-dogbones-f360/},
  urldate = {2022-05-16},
  abstract = {A fast and robust tool for adding dogbone fillets to the inside corners of a model. See the video and the full product description below for complete information about what this add-in does and how to use it.      Click the correct button below to download a fully-functional version of the add-in that will work for 60 days. After 60 days you must purchase a license to continue to use it. Purchasing this product will provide you with a license key that you can use to activate the product so it will continue to run after the 60-day trial period. You DO NOT need to purchase it to use the 60-day trial.  Version 1.4.3 is now available. This version supports the March 2022 and later releases of Fusion 360. To update, or install for the first time, download and install the app using the button below.~ The app is the full version with full capabilities. If you have a trial it will no longer function after the trial period ends. Please let me know if you have any trouble installing or running the app.~ You can see information about what's new in each version at the bottom of the description below.},
  langid = {american},
  organization = {{Ekins Solutions, LLC}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\IPNT9ZWV\\nifty-dogbones-f360.html}
}

@letter{emilmartensEmailExchange2022,
  type = {E-mail},
  title = {Email Exchange On},
  author = {Emil Martens, Lie Jan Roger},
  date = {2022-01-19},
  url = {https://mail.google.com/mail/u/0/#search/sensornor/FMfcgzGmtXFlxVZbFLWfDQDrdXbhNtTl},
  urldate = {2022-05-17},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\IU6QCXRX\\0.html}
}

@online{engelDirectSparseOdometry2016,
  title = {Direct {{Sparse Odometry}}},
  author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
  date = {2016-10-07},
  eprint = {1607.02565},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1607.02565},
  url = {http://arxiv.org/abs/1607.02565},
  urldate = {2022-11-28},
  abstract = {We propose a novel direct sparse visual odometry formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry -- represented as inverse depth in a reference frame -- and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on mostly white walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\RH4I668Y\\Engel et al. - 2016 - Direct Sparse Odometry.pdf;C\:\\Users\\emilm\\Zotero\\storage\\5FYEJKIB\\1607.html}
}

@online{ericPuffedLipoBattery2017,
  title = {Puffed {{Lipo Battery}}: {{Why}} They Swell and What to Do about It â€¢ {{LearningRC}}},
  shorttitle = {Puffed {{Lipo Battery}}},
  author = {Eric},
  date = {2017-03-15T19:02:55+00:00},
  url = {http://learningrc.com/puffed-lipos/},
  urldate = {2022-05-19},
  abstract = {Anybody who uses lipos will eventually encounter a puffy or swollen battery. And the first question that inevitably comes up is "What should I do?"},
  langid = {american},
  organization = {{LearningRC}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\828PQ87W\\puffed-lipos.html}
}

@online{esabknowledgecenterWhatCuttingKerf,
  title = {What Is Cutting Kerf?},
  author = {ESAB KNOWLEDGE CENTER},
  url = {https://www.esabna.com/us/en/education/blog/what-is-cutting-kerf.cfm},
  urldate = {2022-05-18},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\QEMNJAIZ\\what-is-cutting-kerf.html}
}

@software{EthzaslKalibr2022,
  title = {Ethz-Asl/Kalibr},
  date = {2022-05-27T09:01:08Z},
  origdate = {2014-05-29T12:31:48Z},
  url = {https://github.com/ethz-asl/kalibr},
  urldate = {2022-05-30},
  abstract = {The Kalibr visual-inertial calibration toolbox},
  organization = {{ETHZ ASL}},
  keywords = {calibration,calibration-toolbox,camera,imu}
}

@software{fedora13CheckingIf,
  title = {13.3.5.~{{Checking}} If Chrony Is {{Synchronized}}},
  author = {Fedora},
  url = {https://docs.fedoraproject.org/en-US/Fedora/22/html/System_Administrators_Guide/sect-Checking_if_chrony_is_synchronized.html},
  urldate = {2022-05-20},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\M39TTF6S\\sect-Checking_if_chrony_is_synchronized.html}
}

@letter{fisherRe15406LUT2023,
  type = {E-mail},
  title = {Re:[\#\# 15406 \#\#]  {{LUT}} and Debayering on {{TRI050S1-QC}}},
  author = {Fisher, Felix},
  date = {2023-02-01}
}

@online{fisherStaticIPAddresses2021,
  title = {Static {{IP Addresses}}: {{Everything You Need}} to {{Know}}},
  shorttitle = {Static {{IP Addresses}}},
  author = {Fisher, Tim},
  date = {2021-09-17},
  url = {https://www.lifewire.com/what-is-a-static-ip-address-2626012},
  urldate = {2023-05-03},
  abstract = {What is a static IP address? It's any manually configured IP address, sometimes referred to as a fixed IP address. Learn the difference between static vs dynamic IP addresses.},
  langid = {english},
  organization = {{Lifewire}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\3PNFJK3V\\what-is-a-static-ip-address-2626012.html}
}

@online{formlabs20223DPrinting,
  title = {The 2022 {{3D Printing Applications Report}}},
  author = {{formlabs}},
  url = {https://formlabs.drift.click/74c0506e-fddb-4221-b325-3fdaa037a281},
  urldate = {2022-05-18},
  abstract = {Read the new report from Formlabs that examines how a new generation of businesses are using 3D printing.},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\ZVIJ4JPE\\74c0506e-fddb-4221-b325-3fdaa037a281.html}
}

@inproceedings{forsterIMUPreintegrationManifold2015,
  title = {{{IMU Preintegration}} on {{Manifold}} for {{Efficient Visual-Inertial Maximum-a-Posteriori Estimation}}},
  author = {Forster, Christian and Carlone, Luca and Dellaert, Frank and Scaramuzza, Davide},
  date = {2015-07-13},
  doi = {10.15607/RSS.2015.XI.006},
  abstract = {Recent results in monocular visual-inertial navigation (VIN) have shown that optimization-based approaches outperform filtering methods in terms of accuracy due to their capability to relinearize past states. However, the improvement comes at the cost of increased computational complexity. In this paper, we address this issue by preintegrating inertial measurements between selected keyframes. The preintegration allows us to accurately summarize hundreds of inertial measurements into a single relative motion constraint. Our first contribution is a preintegration theory that properly addresses the manifold structure of the rotation group and carefully deals with uncertainty propagation. The measurements are integrated in a local frame, which eliminates the need to repeat the integration when the linearization point changes while leaving the opportunity for belated bias corrections. The second contribution is to show that the preintegrated IMU model can be seamlessly integrated in a visual-inertial pipeline under the unifying framework of factor graphs. This enables the use of a structureless model for visual measurements, further accelerating the computation. The third contribution is an extensive evaluation of our monocular VIN pipeline: experimental results confirm that our system is very fast and demonstrates superior accuracy with respect to competitive state-of-the-art filtering and optimization algorithms, including off-the-shelf systems such as Google Tango.},
  keywords = {lie},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\BDGIXSEV\\Forster et al. - 2015 - IMU Preintegration on Manifold for Efficient Visua.pdf}
}

@article{forsterOnManifoldPreintegrationRealTime2017,
  title = {On-{{Manifold Preintegration}} for {{Real-Time Visual-Inertial Odometry}}},
  author = {Forster, Christian and Carlone, Luca and Dellaert, Frank and Scaramuzza, Davide},
  date = {2017-02},
  journaltitle = {IEEE Transactions on Robotics},
  shortjournal = {IEEE Trans. Robot.},
  volume = {33},
  number = {1},
  eprint = {1512.02363},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1--21},
  issn = {1552-3098, 1941-0468},
  doi = {10.1109/TRO.2016.2597321},
  url = {http://arxiv.org/abs/1512.02363},
  urldate = {2022-10-27},
  abstract = {Current approaches for visual-inertial odometry (VIO) are able to attain highly accurate state estimation via nonlinear optimization. However, real-time optimization quickly becomes infeasible as the trajectory grows over time, this problem is further emphasized by the fact that inertial measurements come at high rate, hence leading to fast growth of the number of variables in the optimization. In this paper, we address this issue by preintegrating inertial measurements between selected keyframes into single relative motion constraints. Our first contribution is a \textbackslash emph\{preintegration theory\} that properly addresses the manifold structure of the rotation group. We formally discuss the generative measurement model as well as the nature of the rotation noise and derive the expression for the \textbackslash emph\{maximum a posteriori\} state estimator. Our theoretical development enables the computation of all necessary Jacobians for the optimization and a-posteriori bias correction in analytic form. The second contribution is to show that the preintegrated IMU model can be seamlessly integrated into a visual-inertial pipeline under the unifying framework of factor graphs. This enables the application of incremental-smoothing algorithms and the use of a \textbackslash emph\{structureless\} model for visual measurements, which avoids optimizing over the 3D points, further accelerating the computation. We perform an extensive evaluation of our monocular \textbackslash VIO pipeline on real and simulated datasets. The results confirm that our modelling effort leads to accurate state estimation in real-time, outperforming state-of-the-art approaches.},
  keywords = {Computer Science - Robotics,lie},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\LXEDJZ63\\Forster et al. - 2017 - On-Manifold Preintegration for Real-Time Visual-In.pdf;C\:\\Users\\emilm\\Zotero\\storage\\3MLWJJQU\\1512.html}
}

@online{FramesNotRecommended2022,
  title = {B Frames Not Recommended in Nvv4l2h265enc Gstreamer Plugin},
  date = {2022-05-09T14:55:04+00:00},
  url = {https://forums.developer.nvidia.com/t/b-frames-not-recommended-in-nvv4l2h265enc-gstreamer-plugin/213833},
  urldate = {2023-05-01},
  abstract = {Running gst-inspect-1.0 nvv4l2h265enc shows:  ...   num-B-Frames        : Number of B Frames between two reference frames (not recommended)(Supported only on Xavier)                         flags: readable, writable, changeable only in NULL or READY state                         Unsigned Integer. Range: 0 - 2 Default: 0 ...  I am wondering why this setting is â€œ(not recommended)â€.},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\LVSU2JME\\213833.html}
}

@online{FramesNotRecommended2022a,
  title = {B Frames Not Recommended in Nvv4l2h265enc Gstreamer Plugin},
  date = {2022-05-09T14:55:04+00:00},
  url = {https://forums.developer.nvidia.com/t/b-frames-not-recommended-in-nvv4l2h265enc-gstreamer-plugin/213833},
  urldate = {2023-05-01},
  abstract = {Running gst-inspect-1.0 nvv4l2h265enc shows:  ...   num-B-Frames        : Number of B Frames between two reference frames (not recommended)(Supported only on Xavier)                         flags: readable, writable, changeable only in NULL or READY state                         Unsigned Integer. Range: 0 - 2 Default: 0 ...  I am wondering why this setting is â€œ(not recommended)â€.},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\AMBGTAPS\\213833.html}
}

@inproceedings{furgaleUnifiedTemporalSpatial2013,
  title = {Unified Temporal and Spatial Calibration for Multi-Sensor Systems},
  booktitle = {2013 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Furgale, Paul and Rehder, Joern and Siegwart, Roland},
  date = {2013-11},
  pages = {1280--1286},
  publisher = {{IEEE}},
  location = {{Tokyo}},
  doi = {10.1109/IROS.2013.6696514},
  url = {http://ieeexplore.ieee.org/document/6696514/},
  urldate = {2022-11-26},
  eventtitle = {2013 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}} 2013)},
  isbn = {978-1-4673-6358-7 978-1-4673-6357-0},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\TR8QE7MI\\Furgale et al. - 2013 - Unified temporal and spatial calibration for multi.pdf}
}

@inproceedings{geigerMEMSIMUAHRS2008,
  title = {{{MEMS IMU}} for {{AHRS}} Applications},
  booktitle = {2008 {{IEEE}}/{{ION Position}}, {{Location}} and {{Navigation Symposium}}},
  author = {Geiger, W. and Bartholomeyczik, J. and Breng, U. and Gutmann, W. and Hafen, M. and Handrich, E. and Huber, M. and Jackle, A. and Kempfer, U. and Kopmann, H. and Kunz, J. and Leinfelder, P. and Ohmberger, R. and Probst, U. and Ruf, M. and Spahlinger, G. and Rasch, A. and Straub-Kalthoff, J. and Stroda, M. and Stumpf, K. and Weber, C. and Zimmermann, M. and Zimmermann, S.},
  date = {2008-05},
  pages = {225--231},
  issn = {2153-3598},
  doi = {10.1109/PLANS.2008.4569973},
  abstract = {Northrop Grumman, LITEF is developing MEMS (micro-electro-mechanical systems) based Inertial Measurement Units (IMU) for future attitude and heading reference systems (AHRS) with a target accuracy of 5 deg/h for the gyroscopes and 2.5 mg for the accelerometers. Within the technology development phase, prototype single axis gyroscopes have been realized and extensively tested for effects including temperature, acoustic and vibration sensitivities. These devices employ micro-machined all-silicon gyroscope sensor chips processed with deep reactive ion etching (DRIE). Silicon fusion bonding ensures pressures smaller than 3middot10-2 mbar. Sophisticated analog electronics and digital signal processing condition the capacitive pick-off signals and realize full closed loop operation. The current results with overall bias error smaller than 2 deg/h to 5 deg/h, scale factor error {$<$}1200 ppm, measurement range {$>$}1000 deg/s and angular random walk {$<$}0.4 radic/vh indicate that stable production of 5 deg/h gyroscopes is realistic. The fabrication technology for capacitive, pendulous accelerometer chips is based on that used for the gyros with only an increase in the enclosed pressure to obtain overcritical damping. Pulse width modulation (PWM) within a digital control loop is used to realize closed loop operation. Accelerometer chips have been tested over temperature with a residual bias error {$<$}2.0 mg and a scale factor error {$<$}1400 ppm. These sensor chips have been integrated into an IMU whereby the power budget and size of the sensor electronics have been optimized. In this paper the salient features of the gyro and accelerometer designs are presented together with an overview of the IMU system architecture. Measurement results, with a focus on environmental characteristics and robustness, are included.},
  eventtitle = {2008 {{IEEE}}/{{ION Position}}, {{Location}} and {{Navigation Symposium}}},
  keywords = {Accelerometers,Acoustic testing,Gyroscopes,Measurement units,Microelectromechanical systems,Micromechanical devices,Prototypes,Pulse width modulation,Semiconductor device measurement,Temperature sensors},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\WRVD8KCR\\Geiger et al. - 2008 - MEMS IMU for AHRS applications.pdf;C\:\\Users\\emilm\\Zotero\\storage\\HQ3Z4AVZ\\4569973.html}
}

@article{getreuerMalvarHeCutlerLinearImage2011,
  title = {Malvar-{{He-Cutler Linear Image Demosaicking}}},
  author = {Getreuer, Pascal},
  date = {2011-08-14},
  journaltitle = {Image Processing On Line},
  volume = {1},
  pages = {83--89},
  issn = {2105-1232},
  doi = {10.5201/ipol.2011.g_mhcd},
  url = {https://www.ipol.im/pub/art/2011/g_mhcd/?utm_source=doi},
  urldate = {2023-02-15},
  abstract = {Image demosaicking (or demosaicing) is the interpolation problem of estimating complete color information for an image that has been captured through a color filter array (CFA), particularly on the Bayer pattern. In this paper we review a simple linear method using 5 x 5 filters, proposed by Malvar, He, and Cutler in 2004, that shows surprisingly good results.},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\V6QDUN7Z\\Getreuer - 2011 - Malvar-He-Cutler Linear Image Demosaicking.pdf}
}

@online{giomettiLinuxPPSWikiLinuxPPS2020,
  title = {{{LinuxPPS}} Wiki [{{LinuxPPS}}]},
  author = {{giometti}},
  date = {2020-11-26},
  url = {http://linuxpps.org/doku.php},
  urldate = {2023-05-04},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\7DNNM6H7\\doku.html}
}

@online{gmvDifferentialGNSSNavipedia2018,
  title = {Differential {{GNSS}} - {{Navipedia}}},
  author = {GMV},
  date = {2018-07-23},
  url = {https://gssc.esa.int/navipedia/index.php/Differential_GNSS},
  urldate = {2022-05-21},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\78WRJE3S\\Differential_GNSS.html}
}

@online{gnatAnswerChangingMetric2017,
  title = {Answer to "{{Changing}} the Metric of an Interface Permanently"},
  author = {Gnat},
  date = {2017-12-25},
  url = {https://unix.stackexchange.com/a/413036},
  urldate = {2022-05-20},
  organization = {{Unix \& Linux Stack Exchange}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\QAWC79SX\\changing-the-metric-of-an-interface-permanently.html}
}

@online{GopStructureGoogle,
  title = {Gop Structure - {{Google Search}}},
  url = {https://www.google.com/search?q=gop+structure&rlz=1C1CHBF_enNO1047NO1047&oq=GOP+structure&aqs=chrome.0.0i512l4j0i10i22i30j0i22i30l5.226j0j4&sourceid=chrome&ie=UTF-8#imgrc=LP4rOEfMTtQP3M},
  urldate = {2023-04-12}
}

@online{GPSGovOther,
  title = {{{GPS}}.Gov: {{Other Global Navigation Satellite Systems}} ({{GNSS}})},
  url = {https://www.gps.gov/systems/gnss/},
  urldate = {2022-05-21},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\HB2UBRGU\\gnss.html}
}

@online{grusinSerialPeripheralInterface,
  title = {Serial {{Peripheral Interface}} ({{SPI}}) - Learn.Sparkfun.Com},
  author = {Grusin, Mike},
  url = {https://learn.sparkfun.com/tutorials/serial-peripheral-interface-spi/all},
  urldate = {2022-05-23},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\SQWQSUMI\\all.html}
}

@online{GStreamNvv4l2h264encBFrames2019,
  title = {{{GStream}}: Nvv4l2h264enc: {{B-Frames}} Display Ordering Is Not Right},
  shorttitle = {{{GStream}}},
  date = {2019-07-11T05:37:01+00:00},
  url = {https://forums.developer.nvidia.com/t/gstream-nvv4l2h264enc-b-frames-display-ordering-is-not-right/77852},
  urldate = {2023-04-11},
  abstract = {This bug requires to enable the new b-frames fix see: [url]https://devtalk.nvidia.com/default/topic/1056338/jetson-nano/gstreamer-h264-encoder-bug-gstv4l2h264enc-h-num-b-frames-is-set-to-boolean-instead-int/post/5359591/\#5359591[/url]  It appears that the nvv4l2h264enc encoder is setting the display timestamps the same as the decoder timestamps.  A strict player will show the lack of proper order of each frame i.e. IPbbPbbPbbâ€¦ instead of IbbPbbPbbâ€¦ (Other players will skip one of the b frames an...},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\VNILYA4M\\77852.html}
}

@article{haavardsholmHandbookVisualSLAM,
  title = {A Handbook in {{Visual SLAM}}},
  author = {Haavardsholm, Trym Vegard},
  pages = {95},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\3ASH5QUG\\Haavardsholm - A handbook in Visual SLAM.pdf}
}

@online{hagemannInferringBiasUncertainty2021,
  title = {Inferring Bias and Uncertainty in Camera Calibration},
  author = {Hagemann, Annika and Knorr, Moritz and Janssen, Holger and Stiller, Christoph},
  date = {2021-07-28},
  eprint = {2107.13484},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2107.13484},
  urldate = {2022-11-25},
  abstract = {Accurate camera calibration is a precondition for many computer vision applications. Calibration errors, such as wrong model assumptions or imprecise parameter estimation, can deteriorate a system's overall performance, making the reliable detection and quantification of these errors critical. In this work, we introduce an evaluation scheme to capture the fundamental error sources in camera calibration: systematic errors (biases) and uncertainty (variance). The proposed bias detection method uncovers smallest systematic errors and thereby reveals imperfections of the calibration setup and provides the basis for camera model selection. A novel resampling-based uncertainty estimator enables uncertainty estimation under non-ideal conditions and thereby extends the classical covariance estimator. Furthermore, we derive a simple uncertainty metric that is independent of the camera model. In combination, the proposed methods can be used to assess the accuracy of individual calibrations, but also to benchmark new calibration algorithms, camera models, or calibration setups. We evaluate the proposed methods with simulations and real cameras.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\45KCNPAY\\Hagemann et al. - 2021 - Inferring bias and uncertainty in camera calibrati.pdf;C\:\\Users\\emilm\\Zotero\\storage\\9LT83DD9\\2107.html}
}

@inreference{HalfprecisionFloatingpointFormat2023,
  title = {Half-Precision Floating-Point Format},
  booktitle = {Wikipedia},
  date = {2023-04-03T16:27:48Z},
  url = {https://en.wikipedia.org/w/index.php?title=Half-precision_floating-point_format&oldid=1148024680},
  urldate = {2023-04-21},
  abstract = {In computing, half precision (sometimes called FP16 or float16) is a binary floating-point computer number format that occupies 16 bits (two bytes in modern computers) in computer memory. It is intended for storage of floating-point values in applications where higher precision is not essential, in particular image processing and neural networks. Almost all modern uses follow the IEEE 754-2008 standard, where the 16-bit base-2 format is referred to as binary16, and the exponent uses 5 bits. This can express values in the range Â±65,504, with the minimum value above 1 being 1 + 1/1024. Depending on the computer, half-precision can be over an order of magnitude faster than double precision, e.g. 550 PFLOPS for half-precision vs 37 PFLOPS for double precision on one cloud provider.},
  langid = {english},
  annotation = {Page Version ID: 1148024680},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\IHEZRLUR\\Half-precision_floating-point_format.html}
}

@article{haugoTTK25ComputerVision,
  title = {{{TTK25}} â€“ {{Computer Vision}} for {{Control}} - 3},
  author = {Haugo, Simen},
  pages = {76},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\ND7NXFQJ\\TTK25 â€“ Computer Vision for Control - Simen Haugo.pdf}
}

@article{haugoTTK25ComputerVisiona,
  title = {{{TTK25}} â€“ {{Computer Vision}} for {{Control}} - 4},
  author = {Haugo, Simen},
  pages = {86},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\RGWCX6N6\\TTK25 â€“ Computer Vision for Control - Simen Haugo.pdf}
}

@article{haugoTTK25ComputerVisionb,
  title = {{{TTK25}} â€“ {{Computer Vision}} for {{Control}} - 6},
  author = {Haugo, Simen},
  pages = {44},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\H6BC4KM9\\TTK25 â€“ Computer Vision for Control - Simen Haugo.pdf}
}

@article{haugoTTK25ComputerVisionc,
  title = {{{TTK25}} â€“ {{Computer Vision}} for {{Control}} - 1},
  author = {Haugo, Simen},
  pages = {149},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\YBM35CCL\\TTK25 â€“ Computer Vision for Control - Simen Haugo.pdf}
}

@article{haugoTTK25ComputerVisiond,
  title = {{{TTK25}} â€“ {{Computer Vision}} for {{Control}} - 2},
  author = {Haugo, Simen},
  pages = {89},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\NL7EGP7C\\TTK25 â€“ Computer Vision for Control - Simen Haugo.pdf}
}

@article{haugoTTK25ComputerVisione,
  title = {{{TTK25}} â€“ {{Computer Vision}} for {{Control}} - 5},
  author = {Haugo, Simen},
  pages = {62},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\Z8VPNANK\\TTK25 â€“ Computer Vision for Control - Simen Haugo.pdf}
}

@online{ibmcloudeducationWhatDockerIBM2021,
  title = {What Is {{Docker}}? | {{IBM}}},
  author = {IBM Cloud Education},
  date = {2021-06-23},
  url = {https://www.ibm.com/cloud/learn/docker},
  urldate = {2022-06-07}
}

@online{ibmIBMDocumentationTCPIP2021,
  title = {{{IBM Documentation}}, {{TCPIP IPv4}} Settings},
  author = {IBM},
  date = {2021-03-01},
  url = {https://www.ibm.com/docs/en/linux-on-systems?topic=tuning-tcpip-ipv4-settings},
  urldate = {2023-05-02},
  abstract = {IBM Documentation.},
  langid = {american},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\BBUIYZI7\\linux-on-systems.html}
}

@online{ibmIBMDocumentationTuning2021,
  title = {IBM Documentation, Tuning your Linux system for more efficient parallel job performance},
  author = {IBM},
  date = {2021-03-05},
  url = {https://www.ibm.com/docs/de/smpi/10.2?topic=mpi-tuning-your-linux-system},
  urldate = {2023-04-27},
  abstract = {The Linux default network and network device settings might not produce optimum throughput (bandwidth) and latency numbers for large parallel jobs. The information that is provided describes how to tune the Linux network and certain network devices for better parallel job performance.},
  langid = {ngerman},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\29GPU545\\10.html}
}

@article{ieeeIEEEStandardsInterpretation2002,
  title = {{{IEEE Standards Interpretation}} for {{IEEE Std}} 802.{{1D}}â„¢-1999, {{IEEE Std}} 802.{{1Q}}â„¢- 1998 and {{IEEE Std}} 802.3â„¢, 2000 {{Edition}}},
  author = {IEEE},
  date = {2002-07-26}
}

@report{intelcorporationIntelDualBand2016,
  title = {{{Intel}}Â® {{Dual Band Wireless-AC}} 8265},
  author = {Intel Corporation},
  date = {2016},
  number = {334064-005},
  url = {https://docs.rs-online.com/4813/A700000006829287.pdf}
}

@online{IntelI350am4Chip,
  title = {Intel {{I350am4 Chip Pcie X4 Rj45 Quad}} 4 {{Port Industrial Network Card Poe Vision Frame Grabber Nics Gigabit Ethernet Lan}} 1000mbps - {{Buy Poe Network Card}},{{I350am4 Chip Network Card}},1000m {{Network Card Product}} on {{Alibaba}}.Com},
  url = {https://www.alibaba.com/product-detail/Intel-I350AM4-Chip-PCIE-X4-RJ45_1600345054974.html?spm=a2756.order-detail-ta-bn-b.0.0.24142fc28OCUh7},
  urldate = {2022-05-30},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\TSCANHGU\\Intel-I350AM4-Chip-PCIE-X4-RJ45_1600345054974.html}
}

@online{IntelI350am4Chipa,
  title = {Intel {{I350am4 Chip Pcie X4 Rj45 Quad}} 4 {{Port Industrial Network Card Poe Vision Frame Grabber Nics Gigabit Ethernet Lan}} 1000mbps - {{Buy Poe Network Card}},{{I350am4 Chip Network Card}},1000m {{Network Card Product}} on {{Alibaba}}.Com},
  url = {https://www.alibaba.com/product-detail/Intel-I350AM4-Chip-PCIE-X4-RJ45_1600345054974.html?spm=a2756.order-detail-ta-bn-b.0.0.24142fc28OCUh7},
  urldate = {2023-04-27},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\X8JU5XDF\\Intel-I350AM4-Chip-PCIE-X4-RJ45_1600345054974.html}
}

@video{itsfiveHowCrimpTNC2011,
  entrysubtype = {video},
  title = {How to {{Crimp TNC Male Connector}} Using {{Crimping}} Tool},
  editor = {{Its Five}},
  editortype = {director},
  date = {2011-09-18},
  url = {https://www.youtube.com/watch?v=wKG7eg2CZNc},
  urldate = {2022-05-26},
  abstract = {MX TNC Plug Connector is coaxial RF connectors. It is used to connect the panels. MX TNC (threaded Neill-Concelman) connector is a threaded version of the BNC Connector. The connector has 50 OHMS impedance and operates best in the 0 --11 GHz frequency spectrum.  MX TNC has better performance than the BNC connector at microwave frequencies. MX TNC connector has been employed in a wide range of radio and wired applications. To Crimp TNC Male Connector You Require RG58 Cable Stripping Tool Crimping Tool TNC Male Connector Step1: Insert The Boot \& Crimping Ring Over The Cable Before Striping It. Step2: Strip The Cable According To The Length Of Connector. Step3: Strip The Center Core of Cable According To Tip's Length. Step4: Insert The Tip \& Solder It For Better Performance. Step5: Crimp The Connector. Product Link: http://mdrelectronics.com/tools.asp Please subscribe to our YOUTUBE Channel: MX Electronics (MDRElex) You can also Like our Facebook Page: http://www.facebook.com/mxelectronics This video is a complete guide, however if you have any questions feel free to comment or send us an email on info@mdrelectronics.com or visit our website www.mdrelectronics.com If You Have Any Complaints, Queries Or Suggestions, Call Us On   (022) 4253 6666 Or You Can Mail Us At info@mdrelectronics.com}
}

@report{itu-tHighEfficiencyVideo,
  title = {High Efficiency Video Coding ({{H}}.265)},
  author = {ITU-T},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\J2YT8L2X\\ITU-T - High efficiency video coding (H.265).pdf}
}

@online{jcmbNTRIPNtripClientPy,
  title = {{{NTRIP}}/{{NtripClient}}.Py at Master Â· Jcmb/{{NTRIP}}},
  author = {{jcmb}},
  url = {https://github.com/jcmb/NTRIP},
  urldate = {2022-05-28},
  abstract = {NTRIP, VRS, IBSS Tools. Contribute to jcmb/NTRIP development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\HJHFRG57\\NTRIP.html}
}

@online{JetpackSwitchTegra2022,
  title = {Jetpack 5.0.1 Switch from {{Tegra}} 23x to 19x Family {{SOC}} in Kernel Configuration},
  date = {2022-07-02T09:27:20+00:00},
  url = {https://forums.developer.nvidia.com/t/jetpack-5-0-1-switch-from-tegra-23x-to-19x-family-soc-in-kernel-configuration/219556},
  urldate = {2023-05-04},
  abstract = {Hi,  I tried to change the kernel configuration setting indicating the family SOC:   Unchecked the Tegra 23x family SOC (downstream options) Checked the Tegra 19x family SOC (downstream options) Save configuration into .config file Execute:  The problem is that the last step fails with many errors such as:  /home/bsplinux/Desktop/Jetpack\_5\_0\_1/Build/Workspace/Linux\_for\_Tegra/source/public/kernel/nvidia/drivers/pinctrl/pinctrl-tegra194.c:448:4: error: â€˜const struct tegra\_pingroupâ€™ has no member n...},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\R3EYEHPQ\\219556.html}
}

@online{JetsonLinuxAPI,
  title = {Jetson {{Linux API Reference}}: {{NvVideoEncoder Class Reference}} | {{NVIDIA Docs}}},
  url = {https://docs.nvidia.com/jetson/l4t-multimedia/classNvVideoEncoder.html#a627a6d299d913c10aa56720d4b690b32},
  urldate = {2023-05-01},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\XD99M687\\classNvVideoEncoder.html}
}

@online{JohnsonCinchBel,
  title = {Johnson - {{Cinch}} - {{Bel}} 2.92 Mm 40 {{GHz}}, 50 {{Ohm}}, 0.048" {{PCB Edge}} Connector | {{3D CAD Model Library}} | {{GrabCAD}}},
  url = {https://grabcad.com/library/johnson-cinch-bel-2-92-mm-40-ghz-50-ohm-0-048-pcb-edge-connector-1},
  urldate = {2022-05-15},
  keywords = {cad\_file},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\2H8LIT7G\\johnson-cinch-bel-2-92-mm-40-ghz-50-ohm-0-048-pcb-edge-connector-1.html}
}

@article{jonesCUDAOPTIMIZATIONTIPS,
  title = {{{CUDA OPTIMIZATION TIPS}}, {{TRICKS AND TECHNIQUES}}},
  author = {Jones, Stephen},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\9X6YDEVW\\Jones - CUDA OPTIMIZATION TIPS, TRICKS AND TECHNIQUES.pdf}
}

@inreference{JPEG2023,
  title = {{{JPEG}}},
  booktitle = {Wikipedia},
  date = {2023-02-15T00:47:27Z},
  url = {https://en.wikipedia.org/w/index.php?title=JPEG&oldid=1139412723#Syntax_and_structure},
  urldate = {2023-02-16},
  abstract = {JPEG ( JAY-peg) is a commonly used method of lossy compression for digital images, particularly for those images produced by digital photography. The degree of compression can be adjusted, allowing a selectable tradeoff between storage size and image quality. JPEG typically achieves 10:1 compression with little perceptible loss in image quality. Since its introduction in 1992, JPEG has been the most widely used image compression standard in the world, and the most widely used digital image format, with several billion JPEG images produced every day as of 2015.The term "JPEG" is an acronym for the Joint Photographic Experts Group, which created the standard in 1992. JPEG was largely responsible for the proliferation of digital images and digital photos across the Internet, and later social media.JPEG compression is used in a number of image file formats. JPEG/Exif is the most common image format used by digital cameras and other photographic image capture devices; along with JPEG/JFIF, it is the most common format for storing and transmitting photographic images on the World Wide Web. These format variations are often not distinguished, and are simply called JPEG. The MIME media type for JPEG is image/jpeg, except in older Internet Explorer versions, which provides a MIME type of image/pjpeg when uploading JPEG images. JPEG files usually have a filename extension of .jpg or .jpeg. JPEG/JFIF supports a maximum image size of 65,535Ã—65,535 pixels, hence up to 4 gigapixels for an aspect ratio of 1:1. In 2000, the JPEG group introduced a format intended to be a successor, JPEG 2000, but it was unable to replace the original JPEG as the dominant image standard.},
  langid = {english},
  annotation = {Page Version ID: 1139412723},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\9EIWDM5L\\JPEG.html}
}

@video{juanaragonRG59MakingTNC2020,
  entrysubtype = {video},
  title = {{{RG59 Making}} a {{TNC}} Connection},
  editor = {{juan aragon}},
  editortype = {director},
  date = {2020-05-16},
  url = {https://www.youtube.com/watch?v=M8Zl1cUe_3Q},
  urldate = {2022-05-26}
}

@online{juicesshJuiceSSHFreeSSH,
  title = {{{JuiceSSH}} - {{Free SSH}} Client for {{Android}}},
  author = {{juiceSSH}},
  url = {https://juicessh.com/},
  urldate = {2022-05-20},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\UPL9JKKT\\juicessh.com.html}
}

@online{kangalowSPIJetsonUsing2020,
  title = {{{SPI}} on {{Jetson}} - {{Using Jetson-IO}}},
  author = {{kangalow}},
  date = {2020-05-04T15:59:23+00:00},
  url = {https://jetsonhacks.com/2020/05/04/spi-on-jetson-using-jetson-io/},
  urldate = {2022-05-20},
  abstract = {Remember when you had to jump through all sorts of hoops to get your NVIDIA Jetson NVIDIA Kit to be able to understand Serial Protocol Interface (SPI) devices? Those days be gone. Looky here: Background The introduction of JetPack 4.3 brings with it a new tool, Jetson-IO. All of the Read more ...},
  langid = {american},
  organization = {{JetsonHacks}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\SWJKQ8KD\\spi-on-jetson-using-jetson-io.html}
}

@online{kartverketBrukerveiledningPosisjonstjenester,
  title = {Brukerveiledning posisjonstjenester},
  author = {Kartverket},
  url = {https://kartverket.no/til-lands/posisjon/brukerveiledning-posisjonstjenester},
  urldate = {2022-05-02},
  langid = {norwegianbokmal},
  organization = {{Kartverket.no}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\8XBUL86E\\brukerveiledning-posisjonstjenester.html}
}

@online{kartverketGuideCPOS,
  title = {Guide to {{CPOS}}},
  author = {Kartverket},
  url = {https://www.kartverket.no/en/on-land/posisjon/guide-to-cpos},
  urldate = {2022-05-02},
  langid = {english},
  organization = {{Kartverket.no}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\ENVAZTKJ\\guide-to-cpos.html}
}

@article{kuphaldtLessonsElectricCircuits2007,
  title = {Lessons {{In Electric Circuits}}},
  author = {Kuphaldt, Tony R},
  date = {2007-11-01},
  volume = {Volume IV -- Digital},
  pages = {517},
  url = {https://www.allaboutcircuits.com/assets/pdf/digital.pdf},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\7TXCZYK8\\Kuphaldt - Lessons In Electric Circuits, Volume IV -- Digita.pdf}
}

@inproceedings{lebedaFixingLocallyOptimized2012,
  title = {Fixing the Locally Optimized {{RANSAC}}},
  author = {Lebeda, Karel and Matas, Jiri and Chum, Ondrej},
  date = {2012-09-01},
  doi = {10.5244/C.26.95},
  abstract = {The paper revisits the problem of local optimization for RANSAC. Improvements of the LO-RANSAC procedure are proposed: a use of truncated quadratic cost function, an introduction of a limit on the number of inliers used for the least squares computation and several implementation issues are addressed. The implementation is made publicly available. Extensive experiments demonstrate that the novel algorithm called LO + -RANSAC is (1) very stable (almost non-random in nature), (2) very precise in a broad range of con-ditions, (3) less sensitive to the choice of inlier-outlier threshold and (4) it offers a sig-nificantly better starting point for bundle adjustment than the Gold Standard method advocated in the Hartley-Zisserman book.},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\6HL7IJEP\\Lebeda et al. - 2012 - Fixing the locally optimized RANSAC.pdf}
}

@online{LinuxRtspclientsinkTest,
  title = {Linux - Rtspclientsink Test Pipeline from Command Line - {{Stack Overflow}}},
  url = {https://stackoverflow.com/questions/69098720/rtspclientsink-test-pipeline-from-command-line},
  urldate = {2023-04-26},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\SGX3WIC5\\rtspclientsink-test-pipeline-from-command-line.html}
}

@online{LongtermVisualLocalization,
  title = {Long-Term Visual Localization Revisited - {{Google Search}}},
  url = {https://www.google.com/search?q=long-term+visual+localization+revisited&rlz=1C1CHBF_enNO1002NO1002&oq=Long-term+visual+localization+revisited&aqs=chrome.0.0i512j0i390l2.325j0j4&sourceid=chrome&ie=UTF-8},
  urldate = {2022-11-27}
}

@online{lorenzkuhnHereAre172021,
  type = {Reddit Post},
  title = {[{{D}}] {{Here}} Are 17 Ways of Making {{PyTorch}} Training Faster â€“ What Did {{I}} Miss?},
  author = {{lorenzkuhn}},
  date = {2021-01-12T13:53:03},
  url = {www.reddit.com/r/MachineLearning/comments/kvs1ex/d_here_are_17_ways_of_making_pytorch_training/},
  urldate = {2022-10-28},
  organization = {{r/MachineLearning}},
  keywords = {ml}
}

@online{loshinWhatSSHSecure,
  title = {What Is {{SSH}} ({{Secure Shell}}) and {{How Does}} It {{Work}}? {{Definition}} from {{TechTarget}}},
  shorttitle = {What Is {{SSH}} ({{Secure Shell}}) and {{How Does}} It {{Work}}?},
  author = {Loshin, Peter},
  url = {https://www.techtarget.com/searchsecurity/definition/Secure-Shell},
  urldate = {2022-05-19},
  abstract = {Learn how the network protocol SSH, or Secure Shell, ensures secure access to a computer over an unsecured network. Read its use cases, history and more.},
  langid = {english},
  organization = {{SearchSecurity}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\UISXZTX2\\Secure-Shell.html}
}

@online{LosslessVideoEncoding2022,
  title = {Lossless Video Encoding on {{Jetson Xavier}} Using {{GStreamer}}},
  date = {2022-03-08T23:30:02+00:00},
  url = {https://forums.developer.nvidia.com/t/lossless-video-encoding-on-jetson-xavier-using-gstreamer/205456},
  urldate = {2023-04-12},
  abstract = {I am trying to save a video on Jetson Xavier NX using OpenCV and Gstreamer.  I have an 8UC3 cv::Mat that I am writing to a mp4 video using a GStreamer pipeline. The pipeline uses nvv4l2h264enc bitrate=8000000 insert-aud=1 insert-sps-pps=1  to encod and uses UYVY format. However, I understand I can be losing information here and I really canâ€™t afford to.  Are there ways to make sure the compression is lossless? I can handle larger sizes of the file.},
  langid = {english},
  organization = {{NVIDIA Developer Forums}}
}

@online{lucascaroloSolidWorks2022Vs2021,
  title = {{{SolidWorks}} 2022 vs {{Fusion}} 360: {{The Differences}}},
  shorttitle = {{{SolidWorks}} 2022 vs {{Fusion}} 360},
  author = {Lucas Carolo},
  date = {2021-08-26T18:58:01+00:00},
  url = {https://all3dp.com/2/fusion-360-vs-solidworks-cad-software-compared-side-by-side/},
  urldate = {2022-05-18},
  abstract = {SolidWorks vs. Fusion 360: Dive straight in to find out all about the differences and which CAD program is best for your needs.},
  langid = {english},
  organization = {{All3DP}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\TXVF87PC\\fusion-360-vs-solidworks-cad-software-compared-side-by-side.html}
}

@online{lucidvisionlabsAppNoteBandwidth2022,
  title = {App {{Note}}: {{Bandwidth}} Sharing in Multi-Camera Systems | {{LUCID Support}} \& {{Help}}},
  shorttitle = {App {{Note}}},
  author = {LUCID Vision Labs},
  date = {2022-04-08T14:25:28+00:00},
  url = {https://support.thinklucid.com/app-note-bandwidth-sharing-in-multi-camera-systems/},
  urldate = {2023-04-27},
  abstract = {This application note will discuss interleaving of packets to address collision of packets and will shed light on various methods for synchronization of cameras.},
  langid = {american},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\MPKQY3FB\\app-note-bandwidth-sharing-in-multi-camera-systems.html}
}

@online{lucidvisionlabsArenaSoftwareDevelopment2020,
  title = {Arena {{Software Development Kit}} ({{SDK}}) {{Documentation}} | {{LUCID Support}} \& {{Help}}},
  author = {LUCID Vision Labs},
  date = {2020-05-07T19:41:44+00:00},
  url = {https://support.thinklucid.com/arena-sdk-documentation/},
  urldate = {2023-05-02},
  langid = {american},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\DCANQQFC\\arena-sdk-documentation.html}
}

@online{lucidvisionlabsDownloadsLUCIDVision,
  title = {Downloads | {{LUCID Vision Labs}}},
  author = {LUCID Vision Labs},
  url = {https://thinklucid.com/downloads-hub/},
  urldate = {2022-05-17},
  abstract = {Download the latest LUCID Arena SDK package along with camera firmware files and technical documentation files.},
  langid = {american},
  keywords = {cad\_file},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\QE8FMGJ8\\downloads-hub.html}
}

@online{lucidvisionlabsJumboFramesLUCID2020,
  title = {Jumbo {{Frames}} | {{LUCID Support}} \& {{Help}}},
  author = {LUCID Vision Labs},
  date = {2020-05-08T19:04:09+00:00},
  url = {https://support.thinklucid.com/documentation/arena-sdk-for-linux/jumbo-frames/},
  urldate = {2023-04-27},
  langid = {american},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\LT6K79GM\\jumbo-frames.html}
}

@report{lucidvisionlabsLUCIDGoingPolarizedWhitePaper2018,
  title = {{{LUCID-Going-Polarized-White-Paper}}},
  author = {LUCID Vision Labs},
  date = {2018},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\DKK7MC9Z\\LUCID-Going-Polarized-White-Paper.pdf}
}

@online{lucidvisionlabsPolarizationExplainedSony2018,
  title = {Polarization {{Explained}}: {{The Sony Polarized Sensor}} - {{LUCID Vision Labs}}},
  shorttitle = {Polarization {{Explained}}},
  author = {LUCID Vision Labs},
  date = {2018-01-27T05:26:14+00:00},
  url = {https://thinklucid.com/tech-briefs/polarization-explained-sony-polarized-sensor/},
  urldate = {2023-05-03},
  abstract = {Sony expands its sensor technology leadership by developing their first polarized sensor with their Polarsens technology. This polarization sensor features an innovative 4 pixel block design that includes 4 unique angled polarizers. Learn more about Sony's IMX250MZR CMOS sensor and its use in our Phoenix and Triton polarized cameras.},
  langid = {american},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\BVGQPJJ6\\polarization-explained-sony-polarized-sensor.html}
}

@online{lucidvisionlabsReceiveBuffers2020,
  title = {Receive {{Buffers}}},
  author = {LUCID Vision Labs},
  date = {2020-05-08T19:11:41+00:00},
  url = {https://support.thinklucid.com/documentation/arena-sdk-for-linux/receive-buffers/},
  urldate = {2023-04-27},
  langid = {american},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\KYT5DFY5\\receive-buffers.html}
}

@online{lucidvisionlabsSettingLLAEthernet2020,
  title = {Setting {{Up LLA}} on the {{Ethernet Adapter}}},
  author = {LUCID Vision Labs},
  date = {2020-05-08T19:56:13+00:00},
  url = {https://support.thinklucid.com/documentation/arena-sdk-for-linux/ip-address-assignment/setting-up-lla-on-the-ethernet-adapter/},
  urldate = {2023-04-27},
  langid = {american},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\2E5YVDDG\\setting-up-lla-on-the-ethernet-adapter.html}
}

@online{lucidvisionlabsTriton0MPPolarization,
  title = {Triton 5.{{0MP Polarization Camera}}, {{Sony}}'s {{IMX264MZR}} / {{MYR CMOS}} | {{LUCID Vision Labs}}},
  author = {LUCID Vision Labs},
  url = {https://thinklucid.com/product/triton-5-0-mp-polarization-model-imx264mzrmyr/},
  urldate = {2022-05-30},
  abstract = {The Triton TRI050S1-P camera features Sony's low-cost IMX264MZR \& IMX264MYR CMOS polarized sensors with their Polarsens on-chip polarization technology. 5 MP global shutter, 2/3", 3.45Âµm, up to 24 FPS over GigE . The sensor has 4 different directional polarizing filters (0Â°, 90Â°, 45Â°, and 135Â°) on every four pixels.},
  langid = {american},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\IJN7G325\\triton-5-0-mp-polarization-model-imx264mzrmyr.html}
}

@online{lucidvisionlabsTritonIndustrialGigE,
  title = {Triton {{Industrial GigE Camera}} | {{LUCID Vision Labs}}},
  author = {LUCID Vision Labs},
  url = {https://thinklucid.com/triton-gige-machine-vision/},
  urldate = {2022-05-17},
  abstract = {The Triton Machine Vision camera - Active sensor alignment for superior optical performance. Lightweight and compact with a 29 x 29 mm size. IP67 Ready. GigE Vision and GenICam compliant machine vision camera designed for all industrial environments with exceptional price performance.},
  langid = {american},
  keywords = {part},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\4GW853EL\\triton-gige-machine-vision.html}
}

@article{luitjensCUDAStreamsBest,
  title = {{{CUDA Streams}}: {{Best Practices}} and {{Common Pitfalls}}},
  author = {Luitjens, Justin},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\KTDT7RAE\\Luitjens - CUDA Streams Best Practices and Common Pitfalls.pdf}
}

@online{lukeThingsYouShould2018,
  title = {4 {{Things You Should Know About Jumbo Frames}} Â· Blue42},
  author = {Luke, Arntz},
  date = {2018-09-13},
  url = {https://blue42.net/networking/4-things-must-know-jumbo-frames/},
  urldate = {2023-04-27},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\5SC97XYY\\4-things-must-know-jumbo-frames.html}
}

@misc{martensPortableSensorRig2022,
  title = {A Portable Sensor Rig for Multi-Sensor Data Aquisition in Maritime Environements},
  author = {Martens, Emil},
  date = {2022-06}
}

@online{martirosSymForceSymbolicComputation2022,
  title = {{{SymForce}}: {{Symbolic Computation}} and {{Code Generation}} for {{Robotics}}},
  shorttitle = {{{SymForce}}},
  author = {Martiros, Hayk and Miller, Aaron and Bucki, Nathan and Solliday, Bradley and Kennedy, Ryan and Zhu, Jack and Dang, Tung and Pattison, Dominic and Zheng, Harrison and Tomic, Teo and Henry, Peter and Cross, Gareth and VanderMey, Josiah and Sun, Alvin and Wang, Samuel and Holtz, Kristen},
  date = {2022-05-06},
  eprint = {2204.07889},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.07889},
  url = {http://arxiv.org/abs/2204.07889},
  urldate = {2022-11-01},
  abstract = {We present SymForce, a library for fast symbolic computation, code generation, and nonlinear optimization for robotics applications like computer vision, motion planning, and controls. SymForce combines the development speed and flexibility of symbolic math with the performance of autogenerated, highly optimized code in C++ or any target runtime language. SymForce provides geometry and camera types, Lie group operations, and branchless singularity handling for creating and analyzing complex symbolic expressions in Python, built on top of SymPy. Generated functions can be integrated as factors into our tangent-space nonlinear optimizer, which is highly optimized for real-time production use. We introduce novel methods to automatically compute tangent-space Jacobians, eliminating the need for bug-prone handwritten derivatives. This workflow enables faster runtime code, faster development time, and fewer lines of handwritten code versus the state-of-the-art. Our experiments demonstrate that our approach can yield order of magnitude speedups on computational tasks core to robotics. Code is available at https://github.com/symforce-org/symforce.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics,Computer Science - Symbolic Computation,lie},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\8NX8VMFS\\Martiros et al. - 2022 - SymForce Symbolic Computation and Code Generation.pdf;C\:\\Users\\emilm\\Zotero\\storage\\CCVK86LB\\2204.html}
}

@inproceedings{matasRandomizedRANSACSequential2005,
  title = {Randomized {{RANSAC}} with Sequential Probability Ratio Test},
  booktitle = {Tenth {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}}'05) {{Volume}} 1},
  author = {Matas, J. and Chum, O.},
  date = {2005-10},
  volume = {2},
  pages = {1727-1732 Vol. 2},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2005.198},
  abstract = {A randomized model verification strategy for RANSAC is presented. The proposed method finds, like RANSAC, a solution that is optimal with user-controllable probability n. A provably optimal model verification strategy is designed for the situation when the contamination of data by outliers is known, i.e. the algorithm is the fastest possible (on average) of all randomized RANSAC algorithms guaranteeing 1 - n confidence in the solution. The derivation of the optimality property is based on Wald's theory of sequential decision making. The R-RANSAC with SPRT which does not require the a priori knowledge of the fraction of outliers and has results close to the optimal strategy is introduced. We show experimentally that on standard test data the method is 2 to 10 times faster than the standard RANSAC and up to 4 times faster than previously published methods.},
  eventtitle = {Tenth {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}}'05) {{Volume}} 1},
  keywords = {Algorithm design and analysis,Computer vision,Contamination,Cost function,Cybernetics,Decision making,Robustness,Sequential analysis,Standards publication,Testing},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\U76EUQ3V\\Matas and Chum - 2005 - Randomized RANSAC with sequential probability rati.pdf;C\:\\Users\\emilm\\Zotero\\storage\\D777A5ZN\\1544925.html}
}

@online{MicroSD3D,
  title = {Micro {{SD}} | {{3D CAD Model Library}} | {{GrabCAD}}},
  url = {https://grabcad.com/library/micro-sd-2},
  urldate = {2022-05-15},
  keywords = {cad\_file},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\YFIHT6NJ\\micro-sd-2.html}
}

@online{mucahittoygarNVIDIAJetsonAGX,
  title = {{{NVIDIA Jetson AGX Xavier}} | {{3D CAD Model Library}} | {{GrabCAD}}},
  author = {MÃ¼cahit Toygar},
  url = {https://grabcad.com/library/nvidia-jetson-agx-xavier-1},
  urldate = {2022-05-17},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\LDIJPR2R\\nvidia-jetson-agx-xavier-1.html}
}

@online{munawarDeepstreamInstallationJetson2022,
  title = {Deepstream Installation on Jetson Devices},
  author = {Munawar, Muhammad Rizwan},
  date = {2022-02-06T14:41:42},
  url = {https://medium.com/nerd-for-tech/deepstream-installation-on-jetson-devices-fb55ace0587f},
  urldate = {2023-05-02},
  abstract = {Deepstream is software development kit (sdk) developed by nvidia, mainly for embedded devices, but we can use it on pc (with gpu support)â€¦},
  langid = {english},
  organization = {{Nerd For Tech}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\S7YXER4D\\deepstream-installation-on-jetson-devices-fb55ace0587f.html}
}

@online{nadelHowUseSmartphone2020,
  title = {How to Use a Smartphone as a Mobile Hotspot},
  author = {Nadel, Brian},
  date = {2020-07-22T03:00-05:00},
  url = {https://www.computerworld.com/article/2499772/how-to-use-a-smartphone-as-a-mobile-hotspot.html},
  urldate = {2022-05-20},
  abstract = {Hereâ€™s everything you need to know about Wi-Fi tethering from your phone.},
  langid = {english},
  organization = {{Computerworld}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\TQ6XY3RE\\how-to-use-a-smartphone-as-a-mobile-hotspot.html}
}

@report{NetworkedTransportRTCM2004,
  title = {Networked {{Transport}} of {{RTCM}} via {{Internet Protocol}}},
  date = {2004},
  number = {1},
  url = {https://gssc.esa.int/wp-content/uploads/2018/07/NtripDocumentation.pdf},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\VAHF4SJR\\NtripDocumentation.pdf}
}

@software{NetworkTimeProtocol2022,
  title = {Network {{Time Protocol}} Daemon - {{ArchWiki}}},
  date = {2022-05-16},
  url = {https://wiki.archlinux.org/title/Network_Time_Protocol_daemon},
  urldate = {2022-05-20},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\2XW7DSYV\\Network_Time_Protocol_daemon.html}
}

@online{newhonghuidafastenernhhdstoreM2M3100pcs,
  title = {M2 {{M3}} 100pcs {{Insert Knurled Nuts Brass Hot Melt Inset Nuts Heating Molding Copper Thread Inserts Nut Free Shipping}} - {{Nuts}} - {{AliExpress}}},
  author = {NEWHONGHUIDA Fastener NHHD Store},
  url = {//www.aliexpress.com/item/4001258499799.html?src=ibdm_d03p0558e02r02&sk=&aff_platform=&aff_trace_key=&af=&cv=&cn=&dp=},
  urldate = {2022-05-18},
  abstract = {Smarter Shopping, Better Living!  Aliexpress.com},
  organization = {{aliexpress.com}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\4M9DLGLI\\4001258499799.html}
}

@online{NTRIPRev1Rev2,
  title = {{{NTRIP Rev1}} versus {{Rev2}} Formats},
  url = {https://www.use-snip.com/kb/knowledge-base/ntrip-rev1-versus-rev2-formats/},
  urldate = {2022-05-02},
  abstract = {This article explains the difference between the NTRIP Client connection format style used in Rev1 (version 1.0) and Rev2 (version 2.0) and provides examples.},
  langid = {american},
  organization = {{SNIP Support}}
}

@online{Nvcodec,
  title = {Nvcodec},
  url = {https://gstreamer.freedesktop.org/documentation/nvcodec/index.html?gi-language=c},
  urldate = {2023-04-11},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\YQINJEL6\\index.html}
}

@online{Nvenc10bitHevc2022,
  title = {Nvenc 10bit Hevc},
  date = {2022-01-04T11:18:09+00:00},
  url = {https://forums.developer.nvidia.com/t/nvenc-10bit-hevc/199535},
  urldate = {2023-04-13},
  abstract = {Do we support the 10bit HEVC encoding?  When I use the command below to do the HEVC 10bit encoding, I get non-correct data analyzed by Elecard.  ./video\_encode crowd\_run\_1920x1080\_420\_50\_100frame.yuv.P010 1920 1080 H265 crowd\_main10.265 -p main10  crowd\_main10.265 (567.5 KB)  As the yuv is too large, I only upload one frame of it! If you need the full, I can send it to your e-mail.  crowd\_run\_1920x1080\_420\_50\_1frame.yuv.P010 (5.9 MB)},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\HHJKA8GQ\\199535.html}
}

@article{nvidiaAcceleratedGStreamerUser,
  title = {Accelerated {{GStreamer User Guide}}},
  author = {NVIDIA},
  langid = {english},
  keywords = {Jetson,Xavier}
}

@online{NVIDIAGstreamerNvvidconv2021,
  title = {{{NVIDIA Gstreamer}} Nvvidconv Question},
  date = {2021-01-17T08:17:43+00:00},
  url = {https://forums.developer.nvidia.com/t/nvidia-gstreamer-nvvidconv-question/166063},
  urldate = {2023-05-01},
  abstract = {Hi all,  Iâ€™m trying to use nvvidconv plugin in my application.  My application uses cuda process, so I need to control gpu memory buffer.  As I know, if I add â€˜(memory:NVMM)â€™ in my gstreamer pipeline, it means the pipeline uses gpu memory.  So I received the data from nvvidconv gstreamerpipeline and passed it to cuda process.  But cuda process could not handle it properly.  Here is my pipeline example.  gst-launch-1.0 v4l2src device=/dev/video0 ! â€œvideo/x-raw, format=(string)UYVY, width=(int)192...},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\CSFKMUBD\\4.html}
}

@report{nvidiaJetsonAGXOrin2022,
  title = {Jetson {{AGX Orin Series}} and {{Jetson AGX Xavier Series Interface Comparison}} and {{Migration}}},
  author = {NVIDIA},
  date = {2022-03},
  number = {DA-10655-001\_v1.1},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\GCTBWPT2\\Jetson_AGX_Orin_Jetson_AGX_Xavier_IF_Comparison_Migration_DA-10655-001_v1.1.pdf}
}

@software{nvidiaJetsonGPIOLinux2022,
  title = {Jetson.{{GPIO}} - {{Linux}} for {{Tegra}}},
  author = {NVIDIA},
  date = {2022-05-12T09:06:29Z},
  origdate = {2019-03-27T19:14:17Z},
  url = {https://github.com/NVIDIA/jetson-gpio},
  urldate = {2022-05-20},
  abstract = {A Python library that enables the use of Jetson's GPIOs},
  organization = {{NVIDIA Corporation}}
}

@report{nvidiaNVIDIAJetsonAGX2020,
  title = {{{NVIDIA Jetson AGX Xavier Developer Kit Carrier Board}}},
  author = {NVIDIA},
  date = {2020-06},
  number = {SP-09778-001\_v2.1},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\BVXZ49SZ\\Jetson_AGX_Xavier_Developer_Kit_Carrier_Board_Specification_SP-09778-001_v2.1.pdf}
}

@online{nvidiaNVIDIAJetsonLinux,
  title = {{{NVIDIA Jetson Linux Developer Guide}} : {{Quick Start}} | {{NVIDIA Docs}}},
  author = {NVIDIA},
  url = {https://docs.nvidia.com/jetson/archives/l4t-archived/l4t-3261/index.html#page/Tegra%20Linux%20Driver%20Package%20Development%20Guide/quick_start.html#wwpID0EAAPNHA},
  urldate = {2023-05-03},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\6FGA94NQ\\index.html}
}

@misc{nvidiaNVIDIAJetsonLinux2023,
  title = {{{NVIDIA Jetson Linux Release Notes Version}} 35.3.1 {{GA}}},
  author = {NVIDIA},
  date = {2023-03},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\5PLJ6HG5\\Jetson_Linux_Release_Notes_r35.3.1.pdf}
}

@software{nvidiaNVIDIASDKManager2019,
  title = {{{NVIDIA SDK Manager}}},
  author = {NVIDIA},
  date = {2019-04-11T09:40:33-07:00},
  url = {https://developer.nvidia.com/nvidia-sdk-manager},
  urldate = {2022-05-20},
  abstract = {Everything You Need to Set Up Your Development Environment NVIDIA SDK Manager provides an end-to-end development environment setup solution for NVIDIAâ€™s DRIVE, Jetson, Clara Holoscan, Rivermax, DOCA and Ethernet Switch SDKs for both host and target devices. .deb Ubuntu .rpm CentOS/RHEL Docker ImageUbuntu 18.04 Docker ImageUbuntu 20.04 SDK Manager User Guide Whatâ€™s New in SDK Manager: Version 1.8: SDK Manager is now available with Docker Image based on Ubuntu 20.04. Added support for Jetson AGX Orin. Added support for NVIDIA Converged Accelerator for DOCA SDK users.},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\XE8NBBYW\\nvidia-sdk-manager.html}
}

@online{nvidiaNVIDIATEGRALINUX,
  title = {{{NVIDIA TEGRA LINUX DRIVER PACKAGE QUICK START GUIDE}}},
  author = {NVIDIA},
  url = {https://developer.download.nvidia.com/embedded/L4T/r32_Release_v1.0/Docs/l4t_quick_start_guide.txt?t=eyJscyI6ImdzZW8iLCJsc2QiOiJodHRwczovL3d3dy5nb29nbGUuY29tLyJ9},
  urldate = {2023-05-03},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\RQMQUM4C\\l4t_quick_start_guide.html}
}

@report{nvidiapinmux,
  title = {{{NVIDIA Jetson AGX Xavier Developer Kit Pinmux}}},
  author = {NVIDIA},
  date = {2022-01-28},
  number = {1.4},
  url = {https://developer.nvidia.com/embedded/downloads#?search=Pin&tx=$product,jetson_agx_xavier},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\VIMU6DA2\\Jetson_AGX_Xavier_Series_Pinmux_Configuration_Template_v1.4.xlsm}
}

@online{nvidiaSDKManager2019,
  title = {{{SDK Manager}}},
  author = {NVIDIA},
  date = {2019-04-11T09:40:33-07:00},
  url = {https://developer.nvidia.com/sdk-manager},
  urldate = {2023-05-04},
  abstract = {An end-to-end development environment setup solution for DRIVE, Jetson, and more. SDKs.},
  langid = {american},
  organization = {{NVIDIA Developer}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\ZPRFN2LK\\sdk-manager.html}
}

@report{nvidiaXavierAGXsysonmodule2021,
  title = {{{NVIDIA Jetson AGX Xavier Series System-on-Module}}},
  author = {NVIDIA},
  date = {2021},
  number = {DS-09654-002\_v1.6},
  url = {https://developer.nvidia.com/jetson-agx-xavier-series-datasheet},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\S94ILIF9\\Jetson-AGX-Xavier-Series-Datasheet_DS09654002v1.6.pdf}
}

@report{nvidiaXavierAGXthermalsDesign2021,
  title = {Jetson {{AGX Xavier Series Thermal Design Guide}}},
  author = {NVIDIA},
  date = {2021-06},
  number = {TDG-08981-001\_v1.3},
  url = {https://developer.nvidia.com/embedded/dlc/jetson-agx-xavier-series-thermal-design-guide},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\TXGTLGGD\\Jetson_AGX_Xavier_Series_Thermal_Design_Guide_TDG-08981-001_v1.3.pdf}
}

@online{NvivafilterDifferentInput2021,
  title = {Nvivafilter: Different Input and Output Buffers},
  shorttitle = {Nvivafilter},
  date = {2021-02-07T20:00:53+00:00},
  url = {https://forums.developer.nvidia.com/t/nvivafilter-different-input-and-output-buffers/167848},
  urldate = {2023-04-11},
  abstract = {Iâ€™m developing a filter (.so library) to be used with gstreamer element nvivafilter  The official example, nvsample\_cudaprocess.cu does some changes over the incoming frame data. That is, only one frame and set of buffers exists, it is used as input data and to store output image. Overwrite on the data buffers is done during the processing of the image.  However, in my case, I can not overwrite the input while processing, I need different data storage for input and output.  I think I must follow...},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\3ASZ7XRL\\7.html}
}

@online{NVMMGstreamer2019,
  title = {{{NVMM}} and {{Gstreamer}}},
  date = {2019-01-21T15:42:50+00:00},
  url = {https://forums.developer.nvidia.com/t/nvmm-and-gstreamer/69642},
  urldate = {2023-03-30},
  abstract = {I have two questions regarding NVMM memory:  First of all, what is NVMM, exactly, in technical terms? What does copying to/from normal memory to NVMM memory and back involve? I am particularly interested in whether copies involve a bus or are normal memory-to-memory copies. Also, what is its relation to CUDA memory? Are they the same things? If there is a reference describing the internals of TX2 architecture, describing how different subsystems communicate, it would greatly help me in understan...},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\28CW9KT8\\3.html}
}

@online{NvsampleCudaprocessSrc2018,
  title = {Nvsample\_cudaprocess\_src with Opencv Compile Error},
  date = {2018-04-06T02:15:06+00:00},
  url = {https://forums.developer.nvidia.com/t/nvsample-cudaprocess-src-with-opencv-compile-error/59892},
  urldate = {2023-03-30},
  abstract = {Hi all ,  I have noticed that for us we can use  gst-launch-1.0 nvcamerasrc fpsRange="30 30" ! \textbackslash{}  'video/x-raw(memory:NVMM), width=(int)3840, height=(int)2160, \textbackslash{}  format=(string)I420, framerate=(fraction)30/1' ! nvtee ! \textbackslash{}  nvivafilter cuda-process=true \textbackslash{}  customer-lib-name="libnvsample\_cudaprocess.so" ! \textbackslash{} 'video/x-raw(memory:NVMM), format=(string)NV12' ! nvoverlaysink -e  to perform pre/post/cuda image process .  Then I want to associate with opencv to do the image process . So I compile my own ...},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\R6FV2RUD\\59892.html}
}

@online{omegaverkstedPricePLAFilament,
  title = {Price of {{PLA}} Filament 10 Gram at {{Omega Verksted}}},
  author = {Omega Verksted},
  url = {https://www.omegav.ntnu.no/komp?search=FDM-3D-Printer+filament-PLA+10+gram},
  urldate = {2022-05-18},
  abstract = {Omega Verksted er en forening for elektronikk- og hobbyinteresserte studenter ved Norges Teknisk-Naturvitenskapelige Universitet (NTNU)},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\CUQ4PVPI\\komp.html}
}

@online{OnlineCRC8CRC16,
  title = {Online {{CRC-8 CRC-16 CRC-32 Calculator}}},
  url = {https://crccalc.com/},
  urldate = {2022-05-16},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\B9GD6H7S\\crccalc.com.html}
}

@online{opensourcehardwareassociationResolutionRedefineSPI,
  title = {A {{Resolution}} to {{Redefine SPI Signal Names}}},
  author = {Open Source Hardware Association},
  url = {https://www.oshwa.org/a-resolution-to-redefine-spi-signal-names/},
  urldate = {2022-05-23},
  abstract = {The electronics industry has been using Master and Slave terminology unabashedly and it needs to stop. Thankfully, the industry is already making a shift.},
  langid = {american},
  organization = {{Open Source Hardware Association}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\GPZ9BQIQ\\a-resolution-to-redefine-spi-signal-names.html}
}

@inproceedings{parkIlluminationChangeRobustness2017,
  title = {Illumination Change Robustness in Direct Visual {{SLAM}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Park, Seonwook and SchÃ¶ps, Thomas and Pollefeys, Marc},
  date = {2017-05},
  pages = {4523--4530},
  doi = {10.1109/ICRA.2017.7989525},
  abstract = {Direct visual odometry and Simultaneous Localization and Mapping (SLAM) methods determine camera poses by means of direct image alignment. This optimizes a photometric cost term based on the Lucas-Kanade method. Many recent works use the brightness constancy assumption in the alignment cost formulation and therefore cannot cope with significant illumination changes. Such changes are especially likely to occur for loop closures in SLAM. Alternatives exist which attempt to match images more robustly. In our paper, we perform a systematic evaluation of real-time capable methods. We determine their accuracy and robustness in the context of odometry and of loop closures, both on real images as well as synthetic datasets with simulated lighting changes. We find that for real images, a Census-based method outperforms the others. We make our new datasets available online.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Brightness,Lighting,Measurement,Optimization,Robustness,Simultaneous localization and mapping,Visualization},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\49CWQBZ9\\Park et al. - 2017 - Illumination change robustness in direct visual SL.pdf;C\:\\Users\\emilm\\Zotero\\storage\\EK4MM4UZ\\7989525.html}
}

@article{parkSpatiotemporalCameraLiDARCalibration2020,
  title = {Spatiotemporal {{Camera-LiDAR Calibration}}: {{A Targetless}} and {{Structureless Approach}}},
  shorttitle = {Spatiotemporal {{Camera-LiDAR Calibration}}},
  author = {Park, Chanoh and Moghadam, Peyman and Kim, Soohwan and Sridharan, Sridha and Fookes, Clinton},
  date = {2020},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2001.06175},
  url = {https://arxiv.org/abs/2001.06175},
  urldate = {2022-11-26},
  abstract = {The demand for multimodal sensing systems for robotics is growing due to the increase in robustness, reliability and accuracy offered by these systems. These systems also need to be spatially and temporally co-registered to be effective. In this paper, we propose a targetless and structureless spatiotemporal camera-LiDAR calibration method. Our method combines a closed-form solution with a modified structureless bundle adjustment where the coarse-to-fine approach does not \{require\} an initial guess on the spatiotemporal parameters. Also, as 3D features (structure) are calculated from triangulation only, there is no need to have a calibration target or to match 2D features with the 3D point cloud which provides flexibility in the calibration process and sensor configuration. We demonstrate the accuracy and robustness of the proposed method through both simulation and real data experiments using multiple sensor payload configurations mounted to hand-held, aerial and legged robot systems. Also, qualitative results are given in the form of a colorized point cloud visualization.},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Robotics (cs.RO)},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\D9MEBM6Q\\Park et al. - 2020 - Spatiotemporal Camera-LiDAR Calibration A Targetl.pdf}
}

@online{PassingNVMMFrames2020,
  title = {Passing {{NVMM}} Frames to {{Gstreamer}} Appsink to Apply Custom Processing},
  date = {2020-07-16T14:03:32+00:00},
  url = {https://forums.developer.nvidia.com/t/passing-nvmm-frames-to-gstreamer-appsink-to-apply-custom-processing/142286},
  urldate = {2023-04-08},
  abstract = {Hello There,   TL;DR  How do you access NVMM buffers from nvarguscamerassrc within an appsink callback for furthrue processing with CUDA?  In Detail Iâ€™m trying to port a computer vision application on to the Jetson Xavier AGX platform.  The use-case focuses on low latency and high throughput so weâ€™re looking to implement an efficient and low overhead processing pipeline using NVIDIAâ€™s libraries.  the basic architecture should look like this  nvarguscamerassrc (memory:NVMM) -{$>$} nvvideoconvert (mem...},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\D5F6SZJB\\142286.html}
}

@online{PETGVsPLA2021,
  title = {{{PETG}} vs {{PLA Filament}}: {{The Differences}}},
  shorttitle = {{{PETG}} vs {{PLA Filament}}},
  date = {2021-04-25T16:56:56+00:00},
  url = {https://all3dp.com/2/petg-vs-pla-3d-printing-filaments-compared/},
  urldate = {2022-05-19},
  abstract = {Learn the differences between PLA and PETG filament to see if your next project calls for durable PETG or environmentally-friendly PLA.},
  langid = {english},
  organization = {{All3DP}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\MLA4Q2W8\\petg-vs-pla-3d-printing-filaments-compared.html}
}

@article{piascoSurveyVisualBasedLocalization2018,
  title = {A Survey on {{Visual-Based Localization}}: {{On}} the Benefit of Heterogeneous Data},
  shorttitle = {A Survey on {{Visual-Based Localization}}},
  author = {Piasco, Nathan and SidibÃ©, DÃ©sirÃ© and Demonceaux, CÃ©dric and Gouet-Brunet, ValÃ©rie},
  date = {2018-02-01},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {74},
  pages = {90--109},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2017.09.013},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320317303448},
  urldate = {2022-11-27},
  abstract = {We are surrounded by plenty of information about our environment. From these multiple sources, numerous data could be extracted: set of images, 3D model, coloured points cloud... When classical localization devices failed (e.g. GPS sensor in cluttered environments), aforementioned data could be used within a localization framework. This is called Visual Based Localization (VBL). Due to numerous data types that can be collected from a scene, VBL encompasses a large amount of different methods. This paper presents a survey about recent methods that localize a visual acquisition system according to a known environment. We start by categorizing VBL methods into two distinct families: indirect and direct localization systems. As the localization environment is almost always dynamic, we pay special attention to methods designed to handle appearances changes occurring in a scene. Thereafter, we highlight methods exploiting heterogeneous types of data. Finally, we conclude the paper with a discussion on promising trends that could permit to a localization system to reach high precision pose estimation within an area as large as possible.},
  langid = {english},
  keywords = {Camera relocalisation,Image-based localization,Pose estimation,Visual geo-localization},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\CY33I5PV\\Piasco et al. - 2018 - A survey on Visual-Based Localization On the bene.pdf;C\:\\Users\\emilm\\Zotero\\storage\\3F4PBITU\\S0031320317303448.html}
}

@report{PicoDatasheet,
  title = {Raspberry {{Pi Pico Datasheet}}},
  author = {Raspberry Pi Trading Ltd},
  date = {2021-11-04},
  number = {150df05-clean},
  url = {https://datasheets.raspberrypi.com/pico/pico-datasheet.pdf},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\HQTNXTRK\\Raspberry Pi Pico Datasheet.pdf}
}

@report{PicoSDK,
  type = {Datasheet},
  title = {Raspberry {{Pi Pico C}}/{{C}}++ {{SDK}}},
  author = {Raspberry Pi Trading Ltd},
  date = {2021-11-04},
  number = {150df05-clean},
  pages = {375},
  url = {https://datasheets.raspberrypi.com/pico/raspberry-pi-pico-c-sdk.pdf},
  urldate = {2022-05-02},
  keywords = {datasheet},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\IYDRHITR\\Raspberry Pi Pico CC++ SDK.pdf}
}

@online{player_onePermanentMountsLinux,
  title = {Permanent Mounts - {{Linux Filesystems}} 101 - {{Block Devices}}},
  author = {{player\_one}},
  url = {https://www.codingame.com/playgrounds/2135/linux-filesystems-101---block-devices/permanent-mounts},
  urldate = {2022-05-11},
  abstract = {Explore this playground and try new concepts right into your browser},
  langid = {english},
  organization = {{CodinGame}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\A8DC89EW\\permanent-mounts.html}
}

@online{PolarizedThermalHybrid2020,
  title = {Polarized {{Thermal Hybrid Cameras}} for {{Improved Detection}} of {{Oil Spills}} | {{Inside Oi}}},
  date = {2020-09-13T13:48:34+00:00},
  url = {https://inside.oceanologyinternational.com/2020/09/13/polarized-thermal-hybrid-cameras-for-improved-detection-of-oil-spills/},
  urldate = {2023-02-06},
  abstract = {LWIR imaging has proven useful in detecting oil spills on water... This article describes testing that was done at Ohmsett and the improved oil detection that resulted from the addition of polarization data to an image.},
  langid = {american},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\CAN85CN8\\polarized-thermal-hybrid-cameras-for-improved-detection-of-oil-spills.html}
}

@online{polymerdatabasePolyMethylMethacrylate,
  title = {Poly(Methyl Methacrylate)},
  author = {{polymerdatabase}},
  url = {https://polymerdatabase.com/polymers/polymethylmethacrylate.html},
  urldate = {2022-05-18},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\TSYL7V7P\\polymethylmethacrylate.html}
}

@article{pomerleauReviewPointCloud2015,
  title = {A {{Review}} of {{Point Cloud Registration Algorithms}} for {{Mobile Robotics}}},
  author = {Pomerleau, FranÃ§ois and Colas, Francis and Siegwart, Roland},
  date = {2015},
  journaltitle = {Foundations and Trends in Robotics},
  volume = {4},
  number = {1},
  pages = {1--104},
  publisher = {{Now Publishers}},
  doi = {10.1561/2300000035},
  url = {https://hal.archives-ouvertes.fr/hal-01178661},
  urldate = {2022-11-27},
  abstract = {The topic of this review is geometric registration in robotics. Registration algorithms associate sets of data into a common coordinate system. They have been used extensively in object reconstruction, inspection, medical application, and localization of mobile robotics. We focus on mobile robotics applications in which point clouds are to be registered. While the underlying principle of those algorithms is simple, many variations have been proposed for many different applications. In this review, we give a historical perspective of the registration problem and show that the plethora of solutions can be organized and differentiated according to a few elements. Accordingly, we present a formalization of geometric registration and cast algorithms proposed in the literature into this framework. Finally, we review a few applications of this framework in mobile robotics that cover different kinds of platforms, environments, and tasks. These examples allow us to study the specific requirements of each use case and the necessary configuration choices leading to the registration implementation. Ultimately, the objective of this review is to provide guidelines for the choice of geometric registration configuration.},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\AJZZZP3X\\Pomerleau et al. - 2015 - A Review of Point Cloud Registration Algorithms fo.pdf}
}

@software{pommereningFingerJoints2022,
  title = {Finger {{Joints}}},
  author = {Pommerening, Florian},
  date = {2022-04-08T14:50:49Z},
  origdate = {2020-04-19T16:12:54Z},
  url = {https://github.com/FlorianPommerening/FingerJoints},
  urldate = {2022-05-16},
  abstract = {Fusion 360 add-in for creating finger joints},
  keywords = {spare-time}
}

@article{prasadAreObjectDetection2020,
  title = {Are {{Object Detection Assessment Criteria Ready}} for {{Maritime Computer Vision}}?},
  author = {Prasad, Dilip K. and Dong, Huixu and Rajan, Deepu and Quek, Chai},
  date = {2020-12},
  journaltitle = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {21},
  number = {12},
  pages = {5295--5304},
  issn = {1558-0016},
  doi = {10.1109/TITS.2019.2954464},
  abstract = {Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime setting. Thus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sight. We discuss the problem of defining assessment metrics suitable for maritime computer vision. We consider new bottom edge proximity metrics as assessment metrics for maritime computer vision. These metrics indicate that existing computer vision approaches are indeed promising for maritime computer vision and can play a foundational role in the emerging field of maritime computer vision.},
  eventtitle = {{{IEEE Transactions}} on {{Intelligent Transportation Systems}}},
  keywords = {Artificial intelligence,Computer vision,Image edge detection,intelligent vehicles,Intelligent vehicles,marine vehicles,Marine vehicles,Object detection,performance evaluation,Performance evaluation,Sensors},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\Q79HJYIP\\Prasad et al. - 2020 - Are Object Detection Assessment Criteria Ready for.pdf;C\:\\Users\\emilm\\Zotero\\storage\\8XYEJL7W\\8911242.html}
}

@article{prasadVideoProcessingElectroOptical2017,
  title = {Video {{Processing From Electro-Optical Sensors}} for {{Object Detection}} and {{Tracking}} in a {{Maritime Environment}}: {{A Survey}}},
  shorttitle = {Video {{Processing From Electro-Optical Sensors}} for {{Object Detection}} and {{Tracking}} in a {{Maritime Environment}}},
  author = {Prasad, Dilip K. and Rajan, Deepu and Rachmawati, Lily and Rajabally, Eshan and Quek, Chai},
  date = {2017-08},
  journaltitle = {IEEE Transactions on Intelligent Transportation Systems},
  shortjournal = {IEEE Trans. Intell. Transport. Syst.},
  volume = {18},
  number = {8},
  pages = {1993--2016},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2016.2634580},
  url = {http://ieeexplore.ieee.org/document/7812788/},
  urldate = {2022-07-19},
  keywords = {Annette\_tip},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\47WJ7NTT\\Prasad et al. - 2017 - Video Processing From Electro-Optical Sensors for .pdf}
}

@report{proto-advantagePCB3005A1ChipQuik,
  title = {{{PCB3005A1 Chip Quik Inc}}. | {{Prototyping}}, {{Fabrication Products}} | {{DigiKey}}},
  author = {Proto-Advantage},
  url = {https://www.digikey.no/en/products/detail/PCB3005A1/PCB3005A1-ND/5978207?utm_medium=email&utm_source=oce&utm_campaign=4251_OCE22RT&utm_content=productdetail_NO&utm_cid=2450381&so=74020630&mkt_tok=MDI4LVNYSy01MDcAAAGCT1YQ4xFcBlSaRHgcqImBJ0KvJf_yvsHkqguNvrlaU2jSdK1hbbj08jW2byTY4v0Q9ucqGbxMjS0s0mF4Hj6_D-SlMnLhWEt83RRt4uUY},
  urldate = {2022-05-03},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\C9KTSAAW\\PCB3005A1.pdf;C\:\\Users\\emilm\\Zotero\\storage\\3QPTPM8U\\5978207.html}
}

@online{PythonWhatLimitations,
  title = {Python - {{What}} Limitations Does the {{GIL}} Impose on Using {{GStreamer}} through {{PyGObject}}? - {{Stack Overflow}}},
  url = {https://stackoverflow.com/questions/70709534/what-limitations-does-the-gil-impose-on-using-gstreamer-through-pygobject},
  urldate = {2023-04-08},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\5AAECDXY\\what-limitations-does-the-gil-impose-on-using-gstreamer-through-pygobject.html}
}

@software{Pyubx22022,
  title = {Pyubx2},
  date = {2022-05-20T14:42:58Z},
  origdate = {2020-10-01T17:12:11Z},
  url = {https://github.com/semuconsulting/pyubx2},
  urldate = {2022-05-28},
  abstract = {Python library for parsing and generating UBX GPS/GNSS protocol messages.},
  organization = {{SEMU Consulting}},
  keywords = {gnss,gps,gps-library,u-blox,ubx,ubx-gps-library,ubx-messages,ubx-parser,ubx-protocol}
}

@article{raguramUSACUniversalFramework2013,
  title = {{{USAC}}: {{A Universal Framework}} for {{Random Sample Consensus}}},
  shorttitle = {{{USAC}}},
  author = {Raguram, Rahul and Chum, Ondrej and Pollefeys, Marc and Matas, Jiri and Frahm, Jan-Michael},
  date = {2013-08},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {35},
  number = {8},
  pages = {2022--2038},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2012.257},
  abstract = {A computational problem that arises frequently in computer vision is that of estimating the parameters of a model from data that have been contaminated by noise and outliers. More generally, any practical system that seeks to estimate quantities from noisy data measurements must have at its core some means of dealing with data contamination. The random sample consensus (RANSAC) algorithm is one of the most popular tools for robust estimation. Recent years have seen an explosion of activity in this area, leading to the development of a number of techniques that improve upon the efficiency and robustness of the basic RANSAC algorithm. In this paper, we present a comprehensive overview of recent research in RANSAC-based robust estimation by analyzing and comparing various approaches that have been explored over the years. We provide a common context for this analysis by introducing a new framework for robust estimation, which we call Universal RANSAC (USAC). USAC extends the simple hypothesize-and-verify structure of standard RANSAC to incorporate a number of important practical and computational considerations. In addition, we provide a general-purpose C++ software library that implements the USAC framework by leveraging state-of-the-art algorithms for the various modules. This implementation thus addresses many of the limitations of standard RANSAC within a single unified package. We benchmark the performance of the algorithm on a large collection of estimation problems. The implementation we provide can be used by researchers either as a stand-alone tool for robust estimation or as a benchmark for evaluating new techniques.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {Algorithm design and analysis,Computational modeling,Context,Data models,Estimation,RANSAC,robust estimation,Robustness,Standards},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\5Y7TV54N\\Raguram et al. - 2013 - USAC A Universal Framework for Random Sample Cons.pdf;C\:\\Users\\emilm\\Zotero\\storage\\2RK53FRS\\6365642.html}
}

@online{RaspberryPiPicoR3,
  title = {Raspberry {{Pi Pico-R3}} | {{3D CAD Model Library}} | {{GrabCAD}}},
  url = {https://grabcad.com/library/raspberry-pi-pico-r3-1},
  urldate = {2022-05-15},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\8YZMSB7U\\raspberry-pi-pico-r3-1.html}
}

@online{redhat10ChangingNetwork,
  title = {10.2. {{Changing Network Kernel Settings Red Hat Enterprise Linux}} 5 | {{Red Hat Customer Portal}}},
  author = {Red Hat},
  url = {https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/tuning_and_optimizing_red_hat_enterprise_linux_for_oracle_9i_and_10g_databases/sect-oracle_9i_and_10g_tuning_guide-adjusting_network_settings-changing_network_kernel_settings},
  urldate = {2023-05-02},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\8PQHU6VM\\sect-oracle_9i_and_10g_tuning_guide-adjusting_network_settings-changing_network_kernel_settings.html}
}

@unpublished{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  date = {2016-05-09},
  eprint = {1506.02640},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1506.02640},
  urldate = {2022-05-02},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,nn,yolo},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\SPZEHMCH\\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf;C\:\\Users\\emilm\\Zotero\\storage\\3VFZJJ3L\\1506.html}
}

@online{ReversePathFiltering,
  title = {Reverse {{Path Filtering}}},
  url = {https://tldp.org/HOWTO/Adv-Routing-HOWTO/lartc.kernel.rpf.html},
  urldate = {2023-05-02},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\4M8E4ENM\\lartc.kernel.rpf.html}
}

@inproceedings{richardsonAprilCalAssistedRepeatable2013,
  title = {{{AprilCal}}: {{Assisted}} and Repeatable Camera Calibration},
  shorttitle = {{{AprilCal}}},
  booktitle = {2013 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Richardson, Andrew and Strom, Johannes and Olson, Edwin},
  date = {2013-11},
  pages = {1814--1821},
  publisher = {{IEEE}},
  location = {{Tokyo}},
  doi = {10.1109/IROS.2013.6696595},
  url = {http://ieeexplore.ieee.org/document/6696595/},
  urldate = {2022-11-25},
  eventtitle = {2013 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}} 2013)},
  isbn = {978-1-4673-6358-7 978-1-4673-6357-0},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\FQB4ZUW4\\Richardson et al. - 2013 - AprilCal Assisted and repeatable camera calibrati.pdf}
}

@online{rosskorskyDXFLaserFusion,
  title = {{{DXF}} for {{Laser}} | {{Fusion}} 360 | {{Autodesk App Store}}},
  author = {Ross Korsky},
  url = {https://apps.autodesk.com/FUSION/en/Detail/Index?id=7634902334100976871&appLang=en&os=Mac},
  urldate = {2022-05-16},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\CC9RQHTS\\Index.html}
}

@report{RP2040Datasheet,
  title = {{{RP2040 Datasheet}}},
  author = {Raspberry Pi Trading Ltd},
  date = {2021-11-04},
  number = {150df05-clean},
  url = {https://datasheets.raspberrypi.com/rp2040/rp2040-datasheet.pdf},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\TWLQHE2D\\rp2040-datasheet.pdf}
}

@online{rscomponentsRoseKriegerConnecting,
  title = {Rose+{{Krieger Connecting Component}}, {{Parallel Clamp}}, Strut Profile 30 Mm | {{RS Components}}},
  author = {RS Components},
  url = {https://no.rs-online.com/web/p/connecting-components/4489615/?sra=pmpn},
  urldate = {2022-05-16},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\87XYKYBK\\4489615.html}
}

@online{rscomponentsRSPROAluminium,
  title = {{{RS PRO Aluminium Strut}}, 40 x 40 Mm, 8mm {{Groove}} , 1000mm {{Length}} | {{RS Components}}},
  author = {RS Components},
  url = {https://no.rs-online.com/web/p/tubing-and-profile-struts/7613319},
  urldate = {2022-05-16},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\4BBQAQWV\\0900766b8157c300.pdf;C\:\\Users\\emilm\\Zotero\\storage\\AXQUFMF8\\7613319.html}
}

@online{rtcmspecialcommitteeno.104RTCM10410Standard,
  title = {{{RTCM}} 10410.1 {{Standard}} for {{Networked Transport}} of {{RTCM}} via {{Internet Protocol}} ({{Ntrip}}) {{Version}} 2.0 with {{Amendment}} 2, {{January}} 12, 2021},
  author = {RTCM SPECIAL COMMITTEE NO.104},
  url = {https://rtcm.myshopify.com/products/rtcm-10410-1-standard-for-networked-transport-of-rtcm-via-internet-protocol-ntrip-version-2-0-with-amendment-1-june-28-2011},
  urldate = {2022-05-02},
  abstract = {Networked Transport of RTCM via Internet Protocol (Ntrip) is an application-level protocol that supports streaming Global Navigation Satellite System (GNSS) data over the Internet. Ntrip is a generic, stateless protocol based on the Hypertext Transfer Protocol HTTP/1.1. The HTTP objects are extended to GNSS data stream},
  organization = {{Radio Technical Commission for Maritime Services}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\ABJ9LDI6\\rtcm-10410-1-standard-for-networked-transport-of-rtcm-via-internet-protocol-ntrip-version-2-0-w.html}
}

@online{RTKLIBOpenSource,
  title = {{{RTKLIB}}: {{An Open Source Program Package}} for {{GNSS Positioning}}},
  url = {http://www.rtklib.com/},
  urldate = {2022-05-30},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\Q6A6GK6N\\www.rtklib.com.html}
}

@online{sadowskiEnablingPPSJetson2020,
  title = {Enabling {{PPS}} on {{Jetson Nano}}},
  author = {Sadowski, Mateusz},
  date = {2020-04-28},
  url = {https://msadowski.github.io/pps-support-jetson-nano/},
  urldate = {2022-05-19},
  abstract = {It took me quite a while to add PPS support on Jetson Nano. Hopefully this tutorial helps speed up the process for you.},
  langid = {english},
  organization = {{msadowski blog}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\3RL8L4SZ\\pps-support-jetson-nano.html}
}

@report{safranSTIM300Datasheet,
  type = {Datasheet},
  title = {{{STIM300 Datasheet}}},
  author = {Safran},
  number = {TS1524 rev.28},
  url = {https://d29ykr7lqkqqrr.cloudfront.net/media/5z5lv25o/ts1524-r28-datasheet-stim300.pdf},
  urldate = {2022-05-02},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\G7C62NM5\\ts1524-r28-datasheet-stim300.pdf}
}

@report{safranSTIM300ProductBrief,
  title = {{{STIM300 Product}} Brief},
  author = {Safran},
  url = {https://d29ykr7lqkqqrr.cloudfront.net/media/vl4jeya3/2022-04-06-product-brief-stim300-a4_high-quality-print.pdf},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\8CXX7KW2\\2022-04-06-product-brief-stim300-a4_high-quality-print.pdf}
}

@online{sainiUnableReadNet2021,
  title = {Unable to Read Net.Core.Rmem\_default inside Container Â· {{Issue}} \#42282 Â· Moby/Moby},
  author = {Saini, Param},
  date = {2021-04-11},
  url = {https://github.com/moby/moby/issues/42282},
  urldate = {2023-05-02},
  abstract = {Description I am running docker/container and set the kernel parameters net.core.rmem\_default at the docker host level. However, inside the container, I am unable to access the net. core.rmem\_defau...},
  langid = {english},
  organization = {{GitHub}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\XB887HFZ\\42282.html}
}

@online{sarlinCoarseFineRobust2019,
  title = {From {{Coarse}} to {{Fine}}: {{Robust Hierarchical Localization}} at {{Large Scale}}},
  shorttitle = {From {{Coarse}} to {{Fine}}},
  author = {Sarlin, Paul-Edouard and Cadena, Cesar and Siegwart, Roland and Dymczyk, Marcin},
  date = {2019-04-08},
  eprint = {1812.03506},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1812.03506},
  url = {http://arxiv.org/abs/1812.03506},
  urldate = {2022-11-27},
  abstract = {Robust and accurate visual localization is a fundamental capability for numerous applications, such as autonomous driving, mobile robotics, or augmented reality. It remains, however, a challenging task, particularly for large-scale environments and in presence of significant appearance changes. State-of-the-art methods not only struggle with such scenarios, but are often too resource intensive for certain real-time applications. In this paper we propose HF-Net, a hierarchical localization approach based on a monolithic CNN that simultaneously predicts local features and global descriptors for accurate 6-DoF localization. We exploit the coarse-to-fine localization paradigm: we first perform a global retrieval to obtain location hypotheses and only later match local features within those candidate places. This hierarchical approach incurs significant runtime savings and makes our system suitable for real-time operation. By leveraging learned descriptors, our method achieves remarkable localization robustness across large variations of appearance and sets a new state-of-the-art on two challenging benchmarks for large-scale localization.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\J5XULVRC\\Sarlin et al. - 2019 - From Coarse to Fine Robust Hierarchical Localizat.pdf;C\:\\Users\\emilm\\Zotero\\storage\\W2A6PB26\\1812.html}
}

@online{sattlerUnderstandingLimitationsCNNbased2019,
  title = {Understanding the {{Limitations}} of {{CNN-based Absolute Camera Pose Regression}}},
  author = {Sattler, Torsten and Zhou, Qunjie and Pollefeys, Marc and Leal-Taixe, Laura},
  date = {2019-03-18},
  eprint = {1903.07504},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1903.07504},
  url = {http://arxiv.org/abs/1903.07504},
  urldate = {2022-11-27},
  abstract = {Visual localization is the task of accurate camera pose estimation in a known scene. It is a key problem in computer vision and robotics, with applications including self-driving cars, Structure-from-Motion, SLAM, and Mixed Reality. Traditionally, the localization problem has been tackled using 3D geometry. Recently, end-to-end approaches based on convolutional neural networks have become popular. These methods learn to directly regress the camera pose from an input image. However, they do not achieve the same level of pose accuracy as 3D structure-based methods. To understand this behavior, we develop a theoretical model for camera pose regression. We use our model to predict failure cases for pose regression techniques and verify our predictions through experiments. We furthermore use our model to show that pose regression is more closely related to pose approximation via image retrieval than to accurate pose estimation via 3D structure. A key result is that current approaches do not consistently outperform a handcrafted image retrieval baseline. This clearly shows that additional research is needed before pose regression algorithms are ready to compete with structure-based methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\2B3UV4PD\\Sattler et al. - 2019 - Understanding the Limitations of CNN-based Absolut.pdf;C\:\\Users\\emilm\\Zotero\\storage\\TKYZQGV6\\1903.html}
}

@online{schneiderGuideUnderstandingLiPo21,
  title = {A {{Guide}} to {{Understanding LiPo Batteries}}},
  author = {Schneider, Brian},
  date = {0021-10-09},
  url = {https://rogershobbycenter.com/lipoguide},
  urldate = {2022-05-29},
  langid = {american},
  organization = {{Roger's Hobby Center}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\DZ6J3ATD\\lipoguide.html}
}

@article{schneiderValidationGeometricModels2009,
  title = {Validation of Geometric Models for Fisheye Lenses},
  author = {Schneider, D. and Schwalbe, E. and Maas, H.-G.},
  date = {2009-05},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  shortjournal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {64},
  number = {3},
  pages = {259--266},
  issn = {09242716},
  doi = {10.1016/j.isprsjprs.2009.01.001},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0924271609000045},
  urldate = {2022-11-27},
  langid = {english}
}

@online{schopsWhyHaving102020,
  title = {Why {{Having}} 10,000 {{Parameters}} in {{Your Camera Model}} Is {{Better Than Twelve}}},
  author = {SchÃ¶ps, Thomas and Larsson, Viktor and Pollefeys, Marc and Sattler, Torsten},
  date = {2020-06-23},
  eprint = {1912.02908},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1912.02908},
  urldate = {2022-11-25},
  abstract = {Camera calibration is an essential first step in setting up 3D Computer Vision systems. Commonly used parametric camera models are limited to a few degrees of freedom and thus often do not optimally fit to complex real lens distortion. In contrast, generic camera models allow for very accurate calibration due to their flexibility. Despite this, they have seen little use in practice. In this paper, we argue that this should change. We propose a calibration pipeline for generic models that is fully automated, easy to use, and can act as a drop-in replacement for parametric calibration, with a focus on accuracy. We compare our results to parametric calibrations. Considering stereo depth estimation and camera pose estimation as examples, we show that the calibration error acts as a bias on the results. We thus argue that in contrast to current common practice, generic models should be preferred over parametric ones whenever possible. To facilitate this, we released our calibration pipeline at https://github.com/puzzlepaint/camera\_calibration, making both easy-to-use and accurate camera calibration available to everyone.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\6NT9XM8J\\SchÃ¶ps et al. - 2020 - Why Having 10,000 Parameters in Your Camera Model .pdf;C\:\\Users\\emilm\\Zotero\\storage\\FQVZVYRI\\1912.html}
}

@online{schweberWhatDoesAnalog2017,
  title = {What Does an Analog Driver/Buffer Do?},
  author = {Schweber, Bill},
  date = {2017-06-06T21:05:22+00:00},
  url = {https://www.analogictips.com/what-does-an-analog-driverbuffer-do/},
  urldate = {2022-05-23},
  abstract = {Although buffers and drivers donâ€™t add functionality to a circuit, these apparently simple interface elements are essential to viable circuit design and operation.},
  langid = {american},
  organization = {{Analog IC Tips}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\HGMWI7FV\\what-does-an-analog-driverbuffer-do.html}
}

@software{SGLUTGnsstk2022,
  title = {{{SGL-UT}}/Gnsstk},
  date = {2022-05-29T15:30:29Z},
  origdate = {2022-05-27T18:34:39Z},
  url = {https://github.com/SGL-UT/gnsstk},
  urldate = {2022-05-30},
  abstract = {The goal of the gnsstk project is to provide an open source library to the satellite navigation community--to free researchers to focus on research, not lower level coding.},
  organization = {{Space and Geophysics Laboratory of ARL:UT at Austin}}
}

@online{shanecccThereAnySpidev2021,
  title = {Is There Any Spidev*.* in {{Jetson AGX}} with Native {{JetPack}} 4.6? - {{Jetson}} \& {{Embedded Systems}} / {{Jetson AGX Xavier}}},
  shorttitle = {Is There Any Spidev*.* in {{Jetson AGX}} with Native {{JetPack}} 4.6?},
  author = {ShaneCCC},
  date = {2021-09-29T02:50:17+00:00},
  url = {https://forums.developer.nvidia.com/t/is-there-any-spidev-in-jetson-agx-with-native-jetpack-4-6/190355/3?u=emil.martens},
  urldate = {2022-05-20},
  abstract = {Insert the spidev module by below command.  sudo modprobe spidev},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\V7V32YA6\\5.html}
}

@online{shgargHOWIncreaseGNNS2020,
  title = {{{HOW}} to {{Increase GNNS}} Timing Module on {{Xavier}}? - {{Jetson}} \& {{Embedded Systems}} / {{Jetson AGX Xavier}}},
  shorttitle = {{{HOW}} to {{Increase GNNS}} Timing Module on {{Xavier}}?},
  author = {{shgarg}},
  date = {2020-01-02T05:57:43+00:00},
  url = {https://forums.developer.nvidia.com/t/how-to-increase-gnns-timing-module-on-xavier/107409/8},
  urldate = {2022-05-20},
  abstract = {AON is always on island of the chip.},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\LEJ6ZDQT\\8.html}
}

@report{siliconimagingincBayerColorConversion,
  title = {Bayer Color Conversion and Processing},
  author = {Silicon Imaging, Inc},
  url = {http://www.siliconimaging.com/Specifications/AN3%20-%20Bayer%20Color%20Processing.PDF},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\55ZHURGI\\AN3 - Bayer Color Processing.pdf}
}

@online{snipRTCMMessageCheat2020,
  title = {An {{RTCM}} 3 Message Cheat Sheet},
  author = {SNIP},
  date = {2020-11-12},
  url = {https://www.use-snip.com/kb/knowledge-base/an-rtcm-message-cheat-sheet/},
  urldate = {2022-05-02},
  abstract = {We are often asked what the most common RTCM version 3 messages are. Here is a handy decoder cheat sheet to answer this. SNIP's message decoder view can also be used.},
  langid = {american},
  organization = {{SNIP Support}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\FNE7SWPV\\an-rtcm-message-cheat-sheet.html}
}

@online{solaMicroLieTheory2021,
  title = {A Micro {{Lie}} Theory for State Estimation in Robotics},
  author = {SolÃ , Joan and Deray, Jeremie and Atchuthan, Dinesh},
  date = {2021-12-08},
  eprint = {1812.01537},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1812.01537},
  urldate = {2022-10-26},
  abstract = {A Lie group is an old mathematical abstract object dating back to the XIX century, when mathematician Sophus Lie laid the foundations of the theory of continuous transformation groups. As it often happens, its usage has spread over diverse areas of science and technology many years later. In robotics, we are recently experiencing an important trend in its usage, at least in the fields of estimation, and particularly in motion estimation for navigation. Yet for a vast majority of roboticians, Lie groups are highly abstract constructions and therefore difficult to understand and to use. This may be due to the fact that most of the literature on Lie theory is written by and for mathematicians and physicists, who might be more used than us to the deep abstractions this theory deals with. In estimation for robotics it is often not necessary to exploit the full capacity of the theory, and therefore an effort of selection of materials is required. In this paper, we will walk through the most basic principles of the Lie theory, with the aim of conveying clear and useful ideas, and leave a significant corpus of the Lie theory behind. Even with this mutilation, the material included here has proven to be extremely useful in modern estimation algorithms for robotics, especially in the fields of SLAM, visual odometry, and the like. Alongside this micro Lie theory, we provide a chapter with a few application examples, and a vast reference of formulas for the major Lie groups used in robotics, including most jacobian matrices and the way to easily manipulate them. We also present a new C++ template-only library implementing all the functionality described here.},
  pubstate = {preprint},
  keywords = {Computer Science - Robotics,lie},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\IJ8GE9ZH\\SolÃ  et al. - 2021 - A micro Lie theory for state estimation in robotic.pdf;C\:\\Users\\emilm\\Zotero\\storage\\M4UPB22V\\1812.html}
}

@article{sonnenbergSerialCommunicationsRS2322018,
  title = {Serial {{Communications RS232}}, {{RS485}}, {{RS422}}},
  author = {Sonnenberg, John},
  date = {2018},
  pages = {6},
  abstract = {Electronic communications is all about interlinking circuits (processors or other integrated circuits) to create a symbiotic system. For those individual circuits to swap information, they must share a common standard communication protocol. Many communication protocols have been designed to achieve data exchange.},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\YGWBUZ8H\\Sonnenberg - 2018 - Serial Communications RS232, RS485, RS422.pdf}
}

@report{sourceiexDegreesProtection,
  title = {Degrees of {{Protection}}},
  author = {Source IEx},
  url = {http://www.sourceiex.com/Catalogs/IP%20Degress%20Testing%20Details.pdf},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\T6MLAANP\\IP Degress Testing Details.pdf}
}

@online{sparkfunOpenLogArtemisHookup,
  title = {{{OpenLog Artemis Hookup Guide}} - Learn.Sparkfun.Com},
  author = {Sparkfun},
  url = {https://learn.sparkfun.com/tutorials/openlog-artemis-hookup-guide},
  urldate = {2022-05-30},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\JIJD26W8\\openlog-artemis-hookup-guide.html}
}

@online{sparkfunSparkFunGPSRTKSMABreakout2022,
  title = {{{SparkFun GPS-RTK-SMA Breakout}} - {{ZED-F9P}} ({{Qwiic}}) - {{GPS-16481}} - {{SparkFun Electronics}}},
  author = {Sparkfun},
  date = {2022},
  url = {https://www.sparkfun.com/products/16481},
  urldate = {2022-05-21},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\WDPWJ666\\16481.html}
}

@online{sparkfunSparkFunOpenLogArtemis,
  title = {{{SparkFun OpenLog Artemis}} - {{DEV-16832}} - {{SparkFun Electronics}}},
  author = {Sparkfun},
  url = {https://www.sparkfun.com/products/16832},
  urldate = {2022-05-30},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\HLIX46U5\\16832.html}
}

@online{spennbergWhatKerfParts2019,
  title = {What Is {{Kerf}}? - {{Parts Badger}} - {{Your Online Machine Shop}}},
  shorttitle = {What Is {{Kerf}}?},
  author = {Spennberg, Brandon},
  date = {2019-12-13T17:32:58+00:00},
  url = {https://parts-badger.com/whats-a-kerf/},
  urldate = {2022-05-18},
  abstract = {Kerf is derived from Middle English "kerf, kirf, kyrf", from Old English "cyrf", from Proto-Germanic "kurbiz", and from Proto-Indo-European "gerb"....},
  langid = {american},
  organization = {{Parts Badger}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\C9A7NSI3\\whats-a-kerf.html}
}

@report{stotonTOP106GNSSL1,
  title = {{{TOP106 GNSS L1}}/{{L2 Multiband Antenna}}},
  author = {STOTON},
  url = {https://cdn.sparkfun.com/assets/b/4/6/d/e/TOP106_GNSS_Antenna.pdf},
  urldate = {2022-05-26}
}

@thesis{sundvollCamerabasedPerceptionSystem,
  title = {A {{Camera-based Perception System}} for {{Autonomous Quadcopter Landing}} on a {{Marine Vessel}}},
  author = {Sundvoll, Thomas},
  abstract = {SmÃ¥, ubemannede luftfartÃ¸yer (UAVer) har tiltrukket seg mye oppmerksomhet de siste Ã¥rene, og et av de mest studerte UAVene er quadcopteret. Et quadcopter gÃ¥r ogsÃ¥ under kategorien VTOL-fartÃ¸y som er et begrep som brukes om fartÃ¸y som kan ta av og lande vertikalt. Dette gjÃ¸r at quadcoptere har en fordel nÃ¥r de opererer i omrÃ¥der med lite plass. Sammen med deres hÃ¸ye manÃ¸vrerbarhet er de et godt vertÃ¸y til Ã¥ utfÃ¸re mange oppgaver, slik som inspeksjon, transport av smÃ¥ pakker og overvÃ¥kning for sÃ¸k- og redningsopperasjoner. For Ã¥ Ã¸ke flygetiden og redusere kostnadene ved Ã¥ manuelt styre slike fartÃ¸y, er det i det siste forsket mye pÃ¥ autonome quadcoptre. Deler av en autonom flytur, og sÃ¦rlig landingen, krever et presist posisjonsestimat. Denne oppgaven undersÃ¸ker et bruksomrÃ¥de hvor landingsplassen er betydelig begrenset nÃ¥r det kommer til stÃ¸rrelse, nemlig Ã¥ lande pÃ¥ et lite, sjÃ¸gÃ¥ende fartÃ¸y. I dette tilfellet kan landingsplassen vÃ¦re omtrent pÃ¥ samme stÃ¸rrelse som quadcopteret selv, noe som krever et posisjonsestimat med enda hÃ¸yere presisjon. I dette tilfellet vil ikke vanlige GPS-mÃ¥linger vÃ¦re presist nok til Ã¥ utfÃ¸re autonom landing. Derfor undersÃ¸ker denne oppgaven bruken av kamera som hovedsensor Ã¥ estimere posisjonen til et quadcopter, med forventning om at dette vil gi et bedre estimat. En landingsplattform er designet og bygget for Ã¥ fungere som landingsplass i eksperimentene. Den er designet for Ã¥ etterligne en standard landingsplattform som vanligvis er Ã¥ finne pÃ¥ sjÃ¸gÃ¥ende fartÃ¸y og marine installasjoner. FartÃ¸yet som til slutt vil bruke landingsplattformen er modellskipet ReVolt som er laget av DNV GL, sÃ¥ designet er tilpasset for at landingsplattformen skal passe til dette spesifikke skipet. Et datasyn-system er utviklet med hovedhensikt Ã¥ estimere quadcopterets posisjon relativt til landingsplattformen. Hovedutfordringen med et datasyn-system pÃ¥ sjÃ¸en er mangelen pÃ¥ faste punkter Ã¥ navigere etter, siden sjÃ¸en er i konstant bevegelse. For Ã¥ lÃ¸se dette problemet er tradisjonelle datasyn-metoder brukt, blant annet fargesegmentering, deteksjon av kanter og deteksjon av hjÃ¸rner, for Ã¥ hente ut allerede kjente kjennetegn pÃ¥ landingsplattformen. Ut fra dette er posisjonen estimert ved bruk av hullkamera-modellen og kjente mÃ¥l pÃ¥ landingsplattformen. Metodene og algoritmene for posisjonsestimatet er utviklet ved bruk av OpenCV-biblioteket i Python, og datasyn-systemet er integrert inn i rammeverket Robot Operating System (ROS). I tillegg er en bestikkregning-modul utviklet for Ã¥ gi et estimat basert pÃ¥ interne mÃ¥linger hos quadcopteret, for bruk nÃ¥r ingen datasyn-estimat er tilgjengelig. Systemet er testet bÃ¥de i en simulator og med et fysisk quadcopter og landingsplattform, med nÃ¸yaktige resultat i simulatoren og lovende, men stÃ¸yfulle resultat med det fysiske quadcopteret. Til slutt er det gitt noen forslag til forbedringer av metodene og fremtidig arbeid pÃ¥ temaet.},
  keywords = {Annette\_tip}
}

@online{SupportingHalfprecisionFloats,
  title = {Supporting Half-Precision Floats Is Really Annoying},
  url = {https://futhark-lang.org/blog/2021-08-05-half-precision-floats.html},
  urldate = {2023-04-17},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\CDD5XWNF\\2021-08-05-half-precision-floats.html}
}

@article{tabbSolvingRobotWorldHandEye2017,
  title = {Solving the {{Robot-World Hand-Eye}}(s) {{Calibration Problem}} with {{Iterative Methods}}},
  author = {Tabb, Amy and Yousef, Khalil M. Ahmad},
  date = {2017-08},
  journaltitle = {Machine Vision and Applications},
  shortjournal = {Machine Vision and Applications},
  volume = {28},
  number = {5-6},
  eprint = {1907.12425},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {569--590},
  issn = {0932-8092, 1432-1769},
  doi = {10.1007/s00138-017-0841-7},
  url = {http://arxiv.org/abs/1907.12425},
  urldate = {2022-11-26},
  abstract = {Robot-world, hand-eye calibration is the problem of determining the transformation between the robot end-effector and a camera, as well as the transformation between the robot base and the world coordinate system. This relationship has been modeled as \$\textbackslash mathbf\{AX\}=\textbackslash mathbf\{ZB\}\$, where \$\textbackslash mathbf\{X\}\$ and \$\textbackslash mathbf\{Z\}\$ are unknown homogeneous transformation matrices. The successful execution of many robot manipulation tasks depends on determining these matrices accurately, and we are particularly interested in the use of calibration for use in vision tasks. In this work, we describe a collection of methods consisting of two cost function classes, three different parameterizations of rotation components, and separable versus simultaneous formulations. We explore the behavior of this collection of methods on real datasets and simulated datasets, and compare to seven other state-of-the-art methods. Our collection of methods return greater accuracy on many metrics as compared to the state-of-the-art. The collection of methods is extended to the problem of robot-world hand-multiple-eye calibration, and results are shown with two and three cameras mounted on the same robot.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\3HGPSCME\\Tabb and Yousef - 2017 - Solving the Robot-World Hand-Eye(s) Calibration Pr.pdf;C\:\\Users\\emilm\\Zotero\\storage\\MS3R5J2J\\1907.html}
}

@inproceedings{taipalmaaHighResolutionWaterSegmentation2019,
  title = {High-{{Resolution Water Segmentation}} for {{Autonomous Unmanned Surface Vehicles}}: A {{Novel Dataset}} and {{Evaluation}}},
  shorttitle = {High-{{Resolution Water Segmentation}} for {{Autonomous Unmanned Surface Vehicles}}},
  booktitle = {2019 {{IEEE}} 29th {{International Workshop}} on {{Machine Learning}} for {{Signal Processing}} ({{MLSP}})},
  author = {Taipalmaa, Jussi and Passalis, Nikolaos and Zhang, Honglei and Gabbouj, Moncef and Raitoharju, Jenni},
  date = {2019-10},
  pages = {1--6},
  issn = {1551-2541},
  doi = {10.1109/MLSP.2019.8918694},
  abstract = {Even though Unmanned Surface Vehicles (USVs) are increasingly used to perform various laborious and expensive offshore tasks, they still require an extensive dedicated crew supporting and ensuring the safety of their operations. The recent developments in computer vision and robotics further fueled the interest on developing autonomous USVs that will overcome the aforementioned limitations, unleashing their full potential. One of the most vital and fundamental tasks in order to automate and ensure the safety of USV operations is to perform water segmentation. Despite the importance of developing such segmentation methods, there is a lack of highresolution publicly available datasets, which are suitable for training and evaluating deep learning methods. The main contribution of this paper is collecting, annotating and releasing a publicly available high-resolution dataset for developing deep learning algorithms for water segmentation in a Nordic lake environment. Furthermore, we adapt a deep learning algorithm previously applied for road segmentation to the water segmentation task. While the algorithm obtains a high accuracy, the results also allow for identifying critical limitations of the approach. Finally, we propose and evaluate a novel lightweight fully convolutional neural network architecture, fully adapted to the needs of water segmentation from highresolution images.},
  eventtitle = {2019 {{IEEE}} 29th {{International Workshop}} on {{Machine Learning}} for {{Signal Processing}} ({{MLSP}})},
  keywords = {Boats,deep learning,fully convolutional networks,Image segmentation,Lakes,Machine learning,Roads,Task analysis,Training,unmanned surface vessel,Water segmentation},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\3LD2ARLP\\Taipalmaa et al. - 2019 - High-Resolution Water Segmentation for Autonomous .pdf;C\:\\Users\\emilm\\Zotero\\storage\\8R5L37GC\\8918694.html}
}

@online{tairaThisRightPlace2019,
  title = {Is {{This The Right Place}}? {{Geometric-Semantic Pose Verification}} for {{Indoor Visual Localization}}},
  shorttitle = {Is {{This The Right Place}}?},
  author = {Taira, Hajime and Rocco, Ignacio and Sedlar, Jiri and Okutomi, Masatoshi and Sivic, Josef and Pajdla, Tomas and Sattler, Torsten and Torii, Akihiko},
  date = {2019-09-02},
  eprint = {1908.04598},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1908.04598},
  url = {http://arxiv.org/abs/1908.04598},
  urldate = {2022-11-27},
  abstract = {Visual localization in large and complex indoor scenes, dominated by weakly textured rooms and repeating geometric patterns, is a challenging problem with high practical relevance for applications such as Augmented Reality and robotics. To handle the ambiguities arising in this scenario, a common strategy is, first, to generate multiple estimates for the camera pose from which a given query image was taken. The pose with the largest geometric consistency with the query image, e.g., in the form of an inlier count, is then selected in a second stage. While a significant amount of research has concentrated on the first stage, there is considerably less work on the second stage. In this paper, we thus focus on pose verification. We show that combining different modalities, namely appearance, geometry, and semantics, considerably boosts pose verification and consequently pose accuracy. We develop multiple hand-crafted as well as a trainable approach to join into the geometric-semantic verification and show significant improvements over state-of-the-art on a very challenging indoor dataset.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\W4E38MHL\\Taira et al. - 2019 - Is This The Right Place Geometric-Semantic Pose V.pdf;C\:\\Users\\emilm\\Zotero\\storage\\3BPVHHJ4\\1908.html}
}

@online{tamiyaTamiyaExtraThin,
  title = {Tamiya {{Extra Thin Cement}} w/{{Brush}} 40ml 300087038 - {{Craft}} Accessories - {{Accessories}} - {{Categories}} - Www.Tamiya.De},
  author = {TAMIYA},
  url = {https://www.tamiya.de/tamiya_en/categories/accessories/craft-accessories/tamiya-extra-thin-cement-wbrush-40ml-300087038-en.html},
  urldate = {2022-05-18},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\RLBTWKLH\\tamiya-extra-thin-cement-wbrush-40ml-300087038-en.html}
}

@software{Tegra194gpioSourceCode2019,
  title = {Tegra194-Gpio.h Source Code [Linux/Include/Dt-Bindings/Gpio/Tegra194-Gpio.h] - {{Woboq Code Browser}}},
  date = {2019-03-29},
  url = {https://code.woboq.org/linux/linux/include/dt-bindings/gpio/tegra194-gpio.h.html},
  urldate = {2022-05-20},
  version = {v5.1-rc2},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\REMBJ673\\tegra194-gpio.h.html}
}

@online{teledyneSettingIPAddress01,
  title = {Setting an IP address for a GigE camera to be recognized in Linux},
  author = {family=Teledyne, given=FLIR, given-i=FLIR},
  date = {0001},
  url = {https://www.flir.com.mx/support-center/iis/machine-vision/knowledge-base/setting-an-ip-address-for-a-gige-camera-to-be-recognized-in-linux/},
  urldate = {2023-05-02},
  langid = {mexican},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\BM4KJ7UI\\setting-an-ip-address-for-a-gige-camera-to-be-recognized-in-linux.html}
}

@online{teledyneSettingPersistentIP,
  title = {Setting a Persistent {{IP}} Address (for All {{GigE Vision}} Cameras)},
  author = {Teledyne},
  url = {https://flir.custhelp.com/app/answers/detail/a_id/3032/~/setting-a-persistent-ip-address-%28for-all-gige-vision-cameras%29},
  urldate = {2023-05-03},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\222NJEHB\\setting-a-persistent-ip-address-(for-all-gige-vision-cameras).html}
}

@online{telegartnerJ01150A0069Telegartner50,
  title = {{{J01150A0069}} | {{Telegartner}} 50\textbackslash{{Omega Right Angle Cable Mount}}, {{SMA Connector}} , {{Plug}} | {{RS Components}}},
  author = {Telegartner},
  url = {https://no.rs-online.com/web/p/coaxial-connectors/1938984},
  urldate = {2022-05-26},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\PI5SD64F\\1938984.html}
}

@report{telegartnerMontageanweisungAssemblyInstruction,
  title = {Montageanweisung / {{Assembly Instruction C04xx}}},
  author = {Telegartner},
  url = {https://docs.rs-online.com/cc0c/0900766b80154af1.pdf},
  urldate = {2022-05-26},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\K6X9RECS\\0900766b80154af1.pdf}
}

@online{tewariAdvancesNeuralRendering2022,
  title = {Advances in {{Neural Rendering}}},
  author = {Tewari, Ayush and Thies, Justus and Mildenhall, Ben and Srinivasan, Pratul and Tretschk, Edgar and Wang, Yifan and Lassner, Christoph and Sitzmann, Vincent and Martin-Brualla, Ricardo and Lombardi, Stephen and Simon, Tomas and Theobalt, Christian and Niessner, Matthias and Barron, Jonathan T. and Wetzstein, Gordon and Zollhoefer, Michael and Golyanik, Vladislav},
  date = {2022-03-30},
  eprint = {2111.05849},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2111.05849},
  url = {http://arxiv.org/abs/2111.05849},
  urldate = {2022-11-28},
  abstract = {Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects...},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\IEHS7BKA\\Tewari et al. - 2022 - Advances in Neural Rendering.pdf;C\:\\Users\\emilm\\Zotero\\storage\\6TB2HDZD\\2111.html}
}

@report{texasinstrumentsDS8921xDifferentialLine2015,
  type = {Datasheet},
  title = {{{DS8921x Differential Line Driver}} and {{Receiver Pair}}},
  author = {Texas Instruments},
  date = {2015-01},
  number = {SNLS374D â€“MAY 1998â€“},
  url = {https://www.digikey.no/en/products/detail/texas-instruments/DS8921AMX-NOPB/366600},
  urldate = {2022-05-12},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\C9SXB89F\\ds8921.pdf;C\:\\Users\\emilm\\Zotero\\storage\\AVR2IYTQ\\366600.html}
}

@book{therrienDecisionEstimationClassification1989,
  title = {Decision, {{Estimation}}, and {{Classification}}: {{An Introduction}} to {{Pattern Recognition}} and {{Related Topics}}},
  shorttitle = {Decision, {{Estimation}}, and {{Classification}}},
  author = {Therrien, Charles W.},
  date = {1989},
  eprint = {rmJIAAAACAAJ},
  eprinttype = {googlebooks},
  publisher = {{Wiley}},
  isbn = {978-0-471-50416-0},
  langid = {english},
  pagetotal = {251},
  keywords = {rudolf\_tip},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\4UJHN646\\Therrien1989-Chap3.pdf;C\:\\Users\\emilm\\Zotero\\storage\\WL46NHSQ\\Therrien1989-38-41.pdf}
}

@online{ThingsFirstNew2023,
  title = {Things to Do First with a New {{J1}}},
  date = {2023-02-27T20:11:46+00:00},
  url = {https://forum.snapmaker.com/t/things-to-do-first-with-a-new-j1/29184},
  urldate = {2023-03-02},
  abstract = {In order to make things easier for new users (and for myself when my J1 will arrive as it seems to take a little longer than expected), this is an attempt to make a list of the hardware-related things to do first when you get your J1, based on the topics I found here in the forum so far. I referenced the forum entries where I got the corresponding information from. Please refer to these entries for details. Feel free to add anyting I might have forgotten!   when you unpack your J1, slowly pull o...},
  langid = {english},
  organization = {{Snapmaker: where creation happens}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\FT7D2YKS\\29184.html}
}

@software{thomsSpidevPythonBindings,
  title = {Spidev: {{Python}} Bindings for {{Linux SPI}} Access through Spidev},
  shorttitle = {Spidev},
  author = {Thoms, Volker},
  url = {http://github.com/doceme/py-spidev},
  urldate = {2022-05-20},
  version = {3.5},
  keywords = {Software Development,System - Hardware,System - Hardware - Hardware Drivers},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\TU7Y4WUU\\spidev.html}
}

@online{todorovHowInstallUse,
  title = {How to {{Install}} and {{Use Chrony}} in {{Linux}}},
  author = {Todorov, Marin},
  url = {https://www.tecmint.com/install-chrony-in-centos-ubuntu-linux/},
  urldate = {2022-05-20},
  abstract = {Chrony is a flexible implementation of the Network Time Protocol (NTP), which is used to synchronize the system clock from different NTP servers, reference clocks or via manual input.},
  langid = {american},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\F7ZI9XIM\\install-chrony-in-centos-ubuntu-linux.html}
}

@article{toftLongTermVisualLocalization2022,
  title = {Long-{{Term Visual Localization Revisited}}},
  author = {Toft, Carl and Maddern, Will and Torii, Akihiko and Hammarstrand, Lars and Stenborg, Erik and Safari, Daniel and Okutomi, Masatoshi and Pollefeys, Marc and Sivic, Josef and Pajdla, Tomas and Kahl, Fredrik and Sattler, Torsten},
  date = {2022-04},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {4},
  pages = {2074--2088},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2020.3032010},
  abstract = {Visual localization enables autonomous vehicles to navigate in their surroundings and augmented reality applications to link virtual to real worlds. Practical visual localization approaches need to be robust to a wide variety of viewing conditions, including day-night changes, as well as weather and seasonal variations, while providing highly accurate six degree-of-freedom (6DOF) camera pose estimates. In this paper, we extend three publicly available datasets containing images captured under a wide variety of viewing conditions, but lacking camera pose information, with ground truth pose information, making evaluation of the impact of various factors on 6DOF camera pose estimation accuracy possible. We also discuss the performance of state-of-the-art localization approaches on these datasets. Additionally, we release around half of the poses for all conditions, and keep the remaining half private as a test set, in the hopes that this will stimulate research on long-term visual localization, learned local image features, and related research areas. Our datasets are available at visuallocalization.net, where we are also hosting a benchmarking server for automatic evaluation of results on the test set. The presented state-of-the-art results are to a large degree based on submissions to our server.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {6DOF pose estimation,benchmark,Benchmark testing,Cameras,long-term localization,relocalization,Robots,Solid modeling,Three-dimensional displays,Trajectory,Visual localization,Visualization},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\Y5BC4KMM\\Toft et al. - 2022 - Long-Term Visual Localization Revisited.pdf;C\:\\Users\\emilm\\Zotero\\storage\\AMNES45A\\9229078.html}
}

@online{tomlogan501EnablingPPSXavier2020,
  title = {Enabling {{PPS}} on {{Xavier AGX}} - {{Jetson}} \& {{Embedded Systems}} / {{Jetson AGX Xavier}}},
  author = {Tomlogan501},
  date = {2020-12-18},
  url = {https://forums.developer.nvidia.com/t/enabling-pps-on-xavier-agx/147762/16?u=emil.martens},
  urldate = {2022-05-20},
  abstract = {Finally I got something for the PPS injection in the AGX Xavier},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\2HY8GH59\\16.html}
}

@article{torrMLESACNewRobust2000,
  title = {{{MLESAC}}: {{A New Robust Estimator}} with {{Application}} to {{Estimating Image Geometry}}},
  shorttitle = {{{MLESAC}}},
  author = {Torr, P. H. S. and Zisserman, A.},
  date = {2000-04-01},
  journaltitle = {Computer Vision and Image Understanding},
  shortjournal = {Computer Vision and Image Understanding},
  volume = {78},
  number = {1},
  pages = {138--156},
  issn = {1077-3142},
  doi = {10.1006/cviu.1999.0832},
  url = {https://www.sciencedirect.com/science/article/pii/S1077314299908329},
  urldate = {2022-11-27},
  abstract = {A new method is presented for robustly estimating multiple view relations from point correspondences. The method comprises two parts. The first is a new robust estimator MLESAC which is a generalization of the RANSAC estimator. It adopts the same sampling strategy as RANSAC to generate putative solutions, but chooses the solution that maximizes the likelihood rather than just the number of inliers. The second part of the algorithm is a general purpose method for automatically parameterizing these relations, using the output of MLESAC. A difficulty with multiview image relations is that there are often nonlinear constraints between the parameters, making optimization a difficult task. The parameterization method overcomes the difficulty of nonlinear constraints and conducts a constrained optimization. The method is general and its use is illustrated for the estimation of fundamental matrices, imageâ€“image homographies, and quadratic transformations. Results are given for both synthetic and real images. It is demonstrated that the method gives results equal or superior to those of previous approaches.},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\TXAMWPHP\\Torr and Zisserman - 2000 - MLESAC A New Robust Estimator with Application to.pdf;C\:\\Users\\emilm\\Zotero\\storage\\Q2VEP8FG\\S1077314299908329.html}
}

@online{triss64738SignalDelaysComponents2021,
  title = {Signal Delays between Components in the {{RP2040}} - {{Raspberry Pi Forums}}},
  author = {{triss64738}},
  date = {2021-06-16},
  url = {https://forums.raspberrypi.com/viewtopic.php?t=325355},
  urldate = {2022-05-21},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\Z49HYEZH\\viewtopic.html}
}

@online{TritonMPPolarized2020,
  title = {Triton 5.0 {{MP Polarized Technical Reference Manual}} | {{LUCID Support}} \& {{Help}}},
  date = {2020-04-18T00:36:08+00:00},
  url = {https://support.thinklucid.com/triton-tri050-pq-polarized/},
  urldate = {2023-04-27},
  langid = {american},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\QEH53LQI\\triton-tri050-pq-polarized.html}
}

@online{TritonMPPolarized2020a,
  title = {Triton 5.0 {{MP Polarized Technical Reference Manual}} | {{LUCID Support}} \& {{Help}}},
  date = {2020-04-18T00:36:08+00:00},
  url = {https://support.thinklucid.com/triton-tri050-pq-polarized/},
  urldate = {2023-04-27},
  langid = {american},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\NHAIK744\\triton-tri050-pq-polarized.html}
}

@online{TritonMPPolarized2020b,
  title = {Triton 5.0 {{MP Polarized Technical Reference Manual}} | {{LUCID Support}} \& {{Help}}},
  date = {2020-04-18T00:36:08+00:00},
  url = {https://support.thinklucid.com/triton-tri050-pq-polarized/},
  urldate = {2023-05-03},
  langid = {american},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\W7JK7NQ7\\triton-tri050-pq-polarized.html}
}

@report{u-bloxGPSEssentialsSatellite2009,
  title = {{{GPS Essentials}} of {{Satellite Navigation}}},
  author = {family=blox, prefix=u-, useprefix=true},
  date = {2009},
  number = {GPS-X-02007-D},
  url = {https://content.u-blox.com/sites/default/files/gps_compendiumgps-x-02007.pdf},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\4INXAH4Y\\gps_compendiumgps-x-02007.pdf}
}

@article{u-bloxZEDF9P04BDataSheet,
  title = {{{ZED-F9P-04B Data}} Sheet},
  author = {family=blox, prefix=u-, useprefix=true},
  pages = {25},
  abstract = {This data sheet describes the ZED-F9P high precision module with multiband GNSS receiver. The module provides multi-band RTK with fast convergence times, reliable performance and easy integration of RTK for fast time-to-market. It has a high update rate for highly dynamic applications and centimeter-level accuracy in a small and energy-efficient module.},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\2FIMBAH9\\ZED-F9P-04B Data sheet.pdf}
}

@report{u-bloxZEDF9PIntegrationManual,
  type = {Datasheet},
  title = {{{ZED-F9P Integration}} Manual},
  author = {family=blox, prefix=u-, useprefix=true},
  number = {UBX-18010802 - R11},
  pages = {119},
  url = {https://content.u-blox.com/sites/default/files/ZED-F9P_IntegrationManual_UBX-18010802.pdf},
  urldate = {2022-05-02},
  abstract = {This document describes the features and application of the ZED-F9P, a multi-band GNSS module with integrated RTK offering centimeter-level accuracy.},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\M4X9KVAU\\ZED-F9P Integration manual.pdf}
}

@report{u-bloxZEDF9PInterfaceDescription,
  type = {Datasheet},
  title = {{{ZED-F9P Interface Description}}},
  author = {family=blox, prefix=u-, useprefix=true},
  number = {UBX-18010854 - R07},
  url = {https://cdn.sparkfun.com/assets/f/7/4/3/5/PM-15136.pdf},
  urldate = {2022-05-02},
  abstract = {The Interface Description describes the UBX (version 27. 11), NMEA and RTCM protocols and serves as a reference manual for the u-blox ZED-F9P high precision positioning receiver.},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\HXCZQB48\\PM-15136.pdf}
}

@report{u-bloxZEDF9PMovingBase,
  type = {Datasheet},
  title = {{{ZED-F9P Moving}} Base Applications},
  author = {family=blox, prefix=u-, useprefix=true},
  number = {UBX-19009093 - R02},
  url = {https://content.u-blox.com/sites/default/files/ZED-F9P-MovingBase_AppNote_%28UBX-19009093%29.pdf},
  urldate = {2022-05-02},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\83PM3ULH\\ZED-F9P-MovingBase_AppNote_(UBX-19009093).pdf}
}

@online{UnableAchievePPS2022,
  title = {Unable to Achieve {{PPS}} with {{Jetpack}} 5.0.2},
  date = {2022-10-27T14:20:04+00:00},
  url = {https://forums.developer.nvidia.com/t/unable-to-achieve-pps-with-jetpack-5-0-2/232101},
  urldate = {2023-05-04},
  abstract = {Hi,  We have previously used PPS with our Jetson Xavier NX (JP4.4) successfully, to get a precise time sync (PPS from GPS to NX). I have now upgraded the NX to Jetpack 5.0.2 and tried to follow the same procedure to enable PPS support, and have been able to get pps0 to show, but not pps1. Pps0 is the ktimer signal, I need the gpio-based one.  Here is what I have done:  Built kernel from source, with the following set in .config:   CONFIG\_PPS=y  CONFIG\_PPS\_CLIENT\_KTIMER=y  CONFIG\_PPS\_CLIENT\_LDISC...},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\RIBQE8VM\\24.html}
}

@online{UnintendedContrastSpread2021,
  title = {Unintended Contrast Spread When Using {{Jetson}}, {{GStreamer}}, and {{H265}} to Encode Videos},
  date = {2021-06-01T06:53:50+00:00},
  url = {https://forums.developer.nvidia.com/t/unintended-contrast-spread-when-using-jetson-gstreamer-and-h265-to-encode-videos/179488},
  urldate = {2023-04-13},
  abstract = {Hi everyone,  While encoding grayscale images into videos using GStreamer on Jetson Xavier AGX, I experienced a shift in brightness values. After some investigation, I found out that a spreading of the contrast occurs: dark pixels became darker, light pixels became lighter.  Does anyone of you also experience this issue? Is there a way, e.g. a setting, to prevent this?},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\WHLX5G2S\\14.html}
}

@online{UsingSharedMemory2013,
  title = {Using {{Shared Memory}} in {{CUDA C}}/{{C}}++},
  date = {2013-01-29T07:18+00:00},
  url = {https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/},
  urldate = {2023-04-14},
  abstract = {In the previous post, I looked at how global memory accesses by a group of threads can be coalesced into a single transaction, and how alignment and stride affect coalescing for various generations ofâ€¦},
  langid = {american},
  organization = {{NVIDIA Technical Blog}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\LYBJW44N\\using-shared-memory-cuda-cc.html}
}

@online{vijayanagarBframesDifferencesUse2020,
  title = {I, {{P}}, and {{B-frames}} - {{Differences}} and {{Use Cases Made Easy}} - {{OTTVerse}}},
  author = {Vijayanagar, Krishna Rao},
  date = {2020-12-14T19:18:05+05:30},
  url = {https://ottverse.com/i-p-b-frames-idr-keyframes-differences-usecases/},
  urldate = {2023-04-12},
  abstract = {I-frames, P-frames, and B-frames are very important in video compression. I-frames help restore quality and resilience, while P \& B-frames improve compression.},
  langid = {american},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\MXRJLCBC\\i-p-b-frames-idr-keyframes-differences-usecases.html}
}

@online{visualstudiocodeDevelopingRemoteMachines2022,
  title = {Developing on {{Remote Machines}} Using {{SSH}} and {{Visual Studio Code}}},
  author = {Visual Studio Code},
  date = {2022-05-05},
  url = {https://code.visualstudio.com/docs/remote/ssh},
  urldate = {2022-05-19},
  abstract = {Developing on Remote Machines or VMs using Visual Studio Code Remote Development and SSH},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\2IVQGSK2\\ssh.html}
}

@online{wangDirectShapeDirectPhotometric2020,
  title = {{{DirectShape}}: {{Direct Photometric Alignment}} of {{Shape Priors}} for {{Visual Vehicle Pose}} and {{Shape Estimation}}},
  shorttitle = {{{DirectShape}}},
  author = {Wang, Rui and Yang, Nan and Stueckler, Joerg and Cremers, Daniel},
  date = {2020-03-09},
  eprint = {1904.10097},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1904.10097},
  url = {http://arxiv.org/abs/1904.10097},
  urldate = {2022-11-28},
  abstract = {Scene understanding from images is a challenging problem encountered in autonomous driving. On the object level, while 2D methods have gradually evolved from computing simple bounding boxes to delivering finer grained results like instance segmentations, the 3D family is still dominated by estimating 3D bounding boxes. In this paper, we propose a novel approach to jointly infer the 3D rigid-body poses and shapes of vehicles from a stereo image pair using shape priors. Unlike previous works that geometrically align shapes to point clouds from dense stereo reconstruction, our approach works directly on images by combining a photometric and a silhouette alignment term in the energy function. An adaptive sparse point selection scheme is proposed to efficiently measure the consistency with both terms. In experiments, we show superior performance of our method on 3D pose and shape estimation over the previous geometric approach and demonstrate that our method can also be applied as a refinement step and significantly boost the performances of several state-of-the-art deep learning based 3D object detectors. All related materials and demonstration videos are available at the project page https://vision.in.tum.de/research/vslam/direct-shape.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\KZ2TYTG4\\Wang et al. - 2020 - DirectShape Direct Photometric Alignment of Shape.pdf;C\:\\Users\\emilm\\Zotero\\storage\\WT7RBMXX\\1904.html}
}

@online{westfwSDKIrqLatency2021,
  title = {C {{SDK}} Irq Latency Is Only  1.06Âµs 200ns - {{Raspberry Pi Forums}}},
  author = {WestfW},
  date = {2021-03-07},
  url = {https://forums.raspberrypi.com/viewtopic.php?p=1832049&sid=316d7821063885d14aa13ffa3b11549f#p1832049},
  urldate = {2022-05-21},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\7L4B2Y64\\viewtopic.html}
}

@online{WhyNotSet2021,
  title = {Why Not Set the Latest Tag Defaultly in Source\_sync.Sh?},
  date = {2021-12-21T09:23:28+00:00},
  url = {https://forums.developer.nvidia.com/t/why-not-set-the-latest-tag-defaultly-in-source-sync-sh/198575},
  urldate = {2023-05-03},
  abstract = {Hi,  In source\_sync.sh, if you run it without -t, you should input TAG or Enter again and again.  On the other hand, if you want to figure out tag names excactly, you should use command git tag -l tegra-l4t* but the repo have not been initialized you cannot get correct output by this command.  So, why not set the latest tag as default? Itâ€™s helpful to users who are not familiar with L4T. On the other hand, set -t parameter to git describe --tags \$(git rev-list --tags --max-count=1) is not harmfu...},
  langid = {english},
  organization = {{NVIDIA Developer Forums}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\ILEED8W3\\198575.html}
}

@online{willsWebVideoServer,
  title = {Web\_video\_server - {{ROS Wiki}}},
  author = {Wills, Mitchell},
  url = {http://wiki.ros.org/web_video_server},
  urldate = {2022-05-30},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\SU8PRLQ3\\web_video_server.html}
}

@article{winterCisco3ComApplied2009,
  title = {Dell: {{Cisco}}: {{3Com}}: {{Applied Micro}}: {{Ethernet Alliance}}: {{NetApp}}: {{Force}} 10: {{Intel}}: {{Qlogic}}:},
  author = {Winter, Robert and Hernandez, Rich and Chawla, Gaurav and Faustini, Anthony and Solder, Carl and Scheibe, Thomas and Law, David and Ayandeh, Siamick and Booth, Brad and Kohl, Blaine and Lavacchia, Charlie and Krishnamurthy, Subi and Karthikeyan, Raja and Multanen, Eric and Wadekar, Manoj},
  date = {2009},
  langid = {english},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\KFY43FLZ\\Winter et al. - 2009 - Dell Cisco 3Com Applied Micro Ethernet Allianc.pdf}
}

@online{wongMEMSIMUsUltimate2017,
  title = {{{MEMS IMUs}}: {{The Ultimate}} in {{Sensor Fusion}}},
  shorttitle = {{{MEMS IMUs}}},
  author = {Wong, William G.},
  date = {2017-12-01},
  url = {https://www.electronicdesign.com/technologies/test-measurement/article/21805896/mems-imus-the-ultimate-in-sensor-fusion},
  urldate = {2022-06-07},
  abstract = {From wearables to UAVs, MEMS inertial measurement units (IMUs) pack the performance needed for todayâ€™s advanced applications.},
  langid = {english},
  organization = {{Electronic Design}},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\QE76CDHW\\mems-imus-the-ultimate-in-sensor-fusion.html}
}

@article{wuketichGeneralizedPlasmocytomaUnusually1963,
  title = {[Generalized plasmocytoma with an unusually high degree of infiltration of the stomach]},
  author = {Wuketich, S. and Maehr, G.},
  date = {1963-03-29},
  journaltitle = {Wiener Klinische Wochenschrift},
  shortjournal = {Wien Klin Wochenschr},
  volume = {75},
  eprint = {14002037},
  eprinttype = {pmid},
  pages = {224--232},
  issn = {0043-5325},
  langid = {german},
  keywords = {Multiple Myeloma,MULTIPLE MYELOMA,Neoplasm Metastasis,NEOPLASM METASTASIS,Plasmacytoma,Stomach Neoplasms,STOMACH NEOPLASMS}
}

@online{X264NumberFrames,
  title = {X264 : Number of {{B}} Frames and Ref [{{Archive}}] - {{Doom9}}'s {{Forum}}},
  url = {http://forum.doom9.org/archive/index.php/t-165490.html},
  urldate = {2023-04-12},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\6IY2HMDQ\\t-165490.html}
}

@inreference{YUV2023,
  title = {{{YUV}}},
  booktitle = {Wikipedia},
  date = {2023-03-07T11:49:52Z},
  url = {https://en.wikipedia.org/w/index.php?title=YUV&oldid=1143386199},
  urldate = {2023-03-23},
  abstract = {YUV is a color model typically used as part of a color image pipeline. It encodes a color image or video taking human perception into account, allowing reduced bandwidth for chrominance components, compared to a "direct" RGB-representation. Historically, the terms YUV and Yâ€²UV were used for a specific analog encoding of color information in television systems. Today, the term YUV is commonly used in the computer industry to describe colorspaces that are encoded using YCbCr. The YUV model defines one luminance component (Y) meaning physical linear-space brightness, and two chrominance components, called U (blue projection) and V (red projection) respectively. It can be used to convert to and from the RGB model, and with different color spaces. The closely related Yâ€²UV model uses the luma component (Yâ€²) â€“ nonlinear perceptual brightness, with the prime symbols (') denoting gamma correction. Yâ€²UV is used in the PAL analogue color TV standard (excluding PAL-N). Previous black-and-white systems used only luma (Yâ€²) information.  Color information (U and V) was added separately via a subcarrier so that a black-and-white receiver would still be able to receive and display a color picture transmission in the receiver's native black-and-white format, with no need for extra transmission bandwidth. As for etymology, Y, Yâ€², U, and V are not abbreviations. The use of the letter Y for luminance can be traced back to the choice of XYZ primaries. This lends itself naturally to the usage of the same letter in luma (Yâ€²), which approximates a perceptually uniform correlate of luminance. Likewise, U and V were chosen to differentiate the U and V axes from those in other spaces, such as the x and y chromaticity space. See the equations below or compare the historical development of the math.},
  langid = {english},
  annotation = {Page Version ID: 1143386199},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\AWNGK7M8\\YUV.html}
}

@article{zhangReferencePoseGeneration2021,
  title = {Reference {{Pose Generation}} for {{Long-term Visual Localization}} via {{Learned Features}} and {{View Synthesis}}},
  author = {Zhang, Zichao and Sattler, Torsten and Scaramuzza, Davide},
  date = {2021-04},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {129},
  number = {4},
  eprint = {2005.05179},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {821--844},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-020-01399-8},
  url = {http://arxiv.org/abs/2005.05179},
  urldate = {2022-11-27},
  abstract = {Visual Localization is one of the key enabling technologies for autonomous driving and augmented reality. High quality datasets with accurate 6 Degree-of-Freedom (DoF) reference poses are the foundation for benchmarking and improving existing methods. Traditionally, reference poses have been obtained via Structure-from-Motion (SfM). However, SfM itself relies on local features which are prone to fail when images were taken under different conditions, e.g., day/ night changes. At the same time, manually annotating feature correspondences is not scalable and potentially inaccurate. In this work, we propose a semi-automated approach to generate reference poses based on feature matching between renderings of a 3D model and real images via learned features. Given an initial pose estimate, our approach iteratively refines the pose based on feature matches against a rendering of the model from the current pose estimate. We significantly improve the nighttime reference poses of the popular Aachen Day-Night dataset, showing that state-of-the-art visual localization methods perform better (up to \$47\textbackslash\%\$) than predicted by the original reference poses. We extend the dataset with new nighttime test images, provide uncertainty estimates for our new reference poses, and introduce a new evaluation criterion. We will make our reference poses and our framework publicly available upon publication.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\PGJA8DE5\\Zhang et al. - 2021 - Reference Pose Generation for Long-term Visual Loc.pdf;C\:\\Users\\emilm\\Zotero\\storage\\4JTQK2PQ\\2005.html}
}

@article{zotero-145,
  type = {article}
}

@online{zouObjectDetection202019,
  title = {Object {{Detection}} in 20 {{Years}}: {{A Survey}}},
  shorttitle = {Object {{Detection}} in 20 {{Years}}},
  author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
  date = {2019-05-15},
  eprint = {1905.05055},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1905.05055},
  urldate = {2022-07-19},
  abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today's object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century's time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
  pubstate = {preprint},
  keywords = {Annette\_tip,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\emilm\\Zotero\\storage\\CDB92B5D\\Zou et al. - 2019 - Object Detection in 20 Years A Survey.pdf;C\:\\Users\\emilm\\Zotero\\storage\\KZQ6GS3Y\\1905.html}
}
