\section{Memory management on Jetson platform}
\label{sec:jx_memory}
On \gls{tegra} platforms, there are different types of memory available for use, including device memory, pageable host memory, pinned memory, and unified memory docs.nvidia.com.
These memory types are allocated on the same physical SoC DRAM but have different accessing and caching behaviors.
Device memory is used for buffers limited to \gls{igpu} access, pageable host memory is used for buffers limited to CPU access, pinned memory is recommended for small buffers, and unified memory is cached on both \gls{igpu} and CPU \cite{nvidiaCUDAFTegra2023}.


\subsection{Memory types}
This section presents the different memoty types available on the \jx.
It builds upon the CUDA for Tegra documentation \cite{nvidiaCUDAFTegra2023}, with a focus on what is relevant for the setup on the \sr, where ther is a \jx with compute cabability of 7.2 \cite{CUDA2023} and no external \gls{dgpu}.
A summary of the different memory types is presented in Table \ref{tab:memory_types}.


\providecommand{\tmpfootnote}{\footnote{Cached where compute capability is greater than or equal to 7.2 \cite
        {nvidiaCUDAFTegra2023}. The \jx has compute capability of 7.2 \cite{CUDA2023} }}
\begin{table}
    \centering
    \begin{tabular}{ |c|c|c| }
        \hline
        \textbf{Memory Type} & \textbf{CPU}            & \textbf{iGPU}           \\
        \hline
        Pageable host memory & Cached                  & Not directly accessible \\
        Device memory        & Not directly accessible & Cached                  \\
        Pinned host memory   & Cached                  & Uncached                \\
        Pageable host memory & Cached                  & Cached                  \\
        \hline
    \end{tabular}
    \caption{Memory types on the \jx \cite{nvidiaCUDAFTegra2023}}
    \label{tab:memory_types}
\end{table}
\subsubsection{Pageable host memory}
Pageable host memory, is the normal type of memory used on the \jx,
e.g. it is what is used allocated with \code{malloc} and used to store normal \py objects.
When memory is pageable the \cpu can control where it is stored, and it can be moved to the \gls{swap} area to free up physical memory \cite[6]{nvidiaCUDAFTegra2023}.
This is why it is not directly accessible by the \gls{igpu}.
If memory is not accesed by the \gls{igpu}, pageable memory should be used \cite[9]{nvidiaCUDAFTegra2023}


\subsubsection{Device memory}
Device memory is memory that is directly accessible by the \gls{igpu} but not the \cpu \cite[5]{nvidiaCUDAFTegra2023}.
It is allocated on the same physical SoC DRAM as other memory types, but cached in a way that is optimized for \gpu usage, making in unacessable on the \cpu \cite[5]{nvidiaCUDAFTegra2023}.


\subsubsection{Pinned host memory}
Pinned memory, also known as page-locked memory, is a type of memory that is directly accessible by both the CPU and the iGPU in \gls{tegra} systems \cite{nvidiaCUDAFTegra2023}.
Pinned memory reduces data transfer overhead between the CPU and iGPU because it can be directly accessed by both \cite{nvidiaCUDAFTegra2023}.
However, it is worth noting that pinned memory can lead to increased memory usage as it is not pageable, and should therefore not be used everywhere \cite[38]{nvidiaCUDABestPractices2023}.
It is also not cached on the \gls{igpu}, making it less efficient if the same data is accessed multiple times \cite{nvidiaCUDAFTegra2023}.
However as is discussed in Section \todo, the data from the camera is read only once, making the following statement relevant:
\say{For large buffers, when the buffer is accessed only once on iGPU in a coalescing manner, performance on iGPU can be as good as unified memory on iGPU.}\cite[10]{nvidiaCUDAFTegra2023}


\subsubsection{Unified memory}
Unified memory is acessable and cached on both the \gls{igpu} and the \cpu, making it a good choice for memory that is repeadetly acessed from both devices \cite[10]{nvidiaCUDAFTegra2023}.
The downside of unified memory, compared to pinned memory is that the overhead from cache maintenance operations \cite[12]{nvidiaCUDAFTegra2023}.
The performance can be improved by manually prefetching data, at the cost of making the code more complex \cite[13]{nvidiaCUDAFTegra2023}.
Unified memory was tested on the \sr, but the performance was worse, compared to pinned memory and using pinned memory resulted in fewer lines and more readable code.


\subsubsection{NVMM memory}
